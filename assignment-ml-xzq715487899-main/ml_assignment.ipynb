{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3NEe4BojNQs",
        "outputId": "539afdb3-d4dd-419d-8303-3e30b3c3b196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.8/dist-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3xyXN7XIOkd",
        "outputId": "0300e364-e283-43ce-bd66-47a3d281a8d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: whoosh in /usr/local/lib/python3.8/dist-packages (2.7.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytrec_eval in /usr/local/lib/python3.8/dist-packages (0.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install whoosh\n",
        "!pip install pytrec_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "CrWhZK_sjJWd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "import random\n",
        "import wget\n",
        "random.seed()\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# Please add necessary imports here\n",
        "import nltk\n",
        "from nltk.stem import *\n",
        "from whoosh import index, writing\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.analysis import *\n",
        "from whoosh.qparser import QueryParser\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "import tempfile\n",
        "import subprocess\n",
        "import pytrec_eval\n",
        "import wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTkqUyNn_5rz",
        "outputId": "3f703831-1b34-4544-f86a-67b6b15ed91b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download required resources\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccMNbLzljJWl",
        "outputId": "eeeb406b-c062-4b64-a279-ed1796395120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  20_newsgroups.zip\n",
            "replace 20_newsgroups/talk.politics.mideast/75895? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "filename = wget.download(\"https://github.com/MIE451-2021/course-datasets/raw/main/20_newsgroups.zip\", \"20_newsgroups.zip\")\n",
        "!unzip 20_newsgroups.zip\n",
        "DATA_DIR = \"20_newsgroups\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAOneB4BjJWq"
      },
      "source": [
        "## Functions from lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "wrs_aC-oofj_"
      },
      "source": [
        "**PLEASE DO NOT CHANGE FUNCTION/CLASS NAMES**\n",
        "\n",
        "**PLEASE DO NOT CHANGE/OVERWRITE COMPLETED FUNCTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isVIWKz5jJWr"
      },
      "outputs": [],
      "source": [
        "def clean_file_text(text):\n",
        "    new_text = re.sub(\"Newsgroups:.*?\\n\", \"\", text)\n",
        "    new_text = re.sub(\"Xref:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"Path:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"Date:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"Followup-To:.*?\\n\", \"\", new_text)\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HHTRr8CjJWv"
      },
      "outputs": [],
      "source": [
        "def corpus_count_words(file_list):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    word_counter = Counter()\n",
        "    for file_path in file_list:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_data = file.read()\n",
        "            file_data = clean_file_text(file_data)\n",
        "            file_words = tokenizer.tokenize(file_data)\n",
        "            word_counter.update(file_words)\n",
        "    return word_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VBp_7RBjJW1"
      },
      "outputs": [],
      "source": [
        "def get_topic_name(file_path):\n",
        "    return file_path.parent.name\n",
        "\n",
        "def get_target(topic_name):\n",
        "    topics = ['talk.politics.mideast', 'rec.autos', 'comp.sys.mac.hardware', 'alt.atheism', 'rec.sport.baseball', \n",
        "     'comp.os.ms-windows.misc', 'rec.sport.hockey', 'sci.crypt', 'sci.med', 'talk.politics.misc', \n",
        "     'rec.motorcycles', 'comp.windows.x', 'comp.graphics', 'comp.sys.ibm.pc.hardware', 'sci.electronics',\n",
        "     'talk.politics.guns', 'sci.space', 'soc.religion.christian', 'misc.forsale', 'talk.religion.misc']\n",
        "    return topics.index(topic_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH81rhWjjJW4"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm):\n",
        "    # plot the confusion matrix\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.matshow(cm, fignum=1)\n",
        "    \n",
        "    # add labels for all targets\n",
        "    num_targets = cm.shape[0]\n",
        "    plt.xticks(list(range(num_targets+1)))\n",
        "    plt.yticks(list(range(num_targets+1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcSg1fnjzKWN"
      },
      "outputs": [],
      "source": [
        "from whoosh.analysis import Filter\n",
        "class CustomFilter(Filter):\n",
        "    is_morph = True\n",
        "    def __init__(self, filterFunc, *args, **kwargs):\n",
        "        self.customFilter = filterFunc\n",
        "        self.args = args\n",
        "        self.kwargs = kwargs\n",
        "    def __eq__(self):\n",
        "        return (other\n",
        "                and self.__class__ is other.__class__)\n",
        "    def __call__(self, tokens):\n",
        "        for t in tokens:\n",
        "            if t.mode == 'query': # if called by query parser\n",
        "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
        "                yield t\n",
        "            else: # == 'index' if called by indexer\n",
        "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
        "                yield t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78-8XL-ozP3V"
      },
      "outputs": [],
      "source": [
        "def corpus_count_words_V2(file_list):\n",
        "  \n",
        "  word_counter = Counter()\n",
        "  for file_path in file_list:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_data = file.read()\n",
        "            file_data = clean_file_text(file_data)\n",
        "            # Use several techniques to improve the text analyzing performance\n",
        "            NewAnalyzer = RegexTokenizer(r'\\w+') | LowercaseFilter()| StopFilter() | StemFilter()|CustomFilter(WordNetLemmatizer().lemmatize)\n",
        "            file_words = [token.text for token in NewAnalyzer(file_data)]\n",
        "            word_counter.update(file_words)\n",
        "  return word_counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "6d0H1MoeofkB"
      },
      "source": [
        "## Q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vn5BiAdrofkB"
      },
      "outputs": [],
      "source": [
        "class MLQ1():\n",
        "    def binary_baseline_data(self, file_list, num_words=1000):\n",
        "        # Calculate word count in corpus\n",
        "        news_cnt = corpus_count_words(file_list)\n",
        "\n",
        "        # Select the most common numWords\n",
        "        word_list = [word for (word, freq) in news_cnt.most_common(num_words)]\n",
        "\n",
        "        # Create a binary encoding of dataset based on the selected features (X)\n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        df_rows = []\n",
        "        for file_path in file_list:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                file_data = file.read()\n",
        "                file_data = clean_file_text(file_data)\n",
        "                file_words = tokenizer.tokenize(file_data)\n",
        "                df_rows.append([1 if word in file_words else 0 for word in word_list])\n",
        "        X = pd.DataFrame(df_rows, index=[str(f) for f in file_list], columns = word_list)\n",
        "\n",
        "        # Create a dataframe of targets (y)\n",
        "        y = [get_target(get_topic_name(file_path)) for file_path in file_list]\n",
        "\n",
        "        return X, y\n",
        "        \n",
        "\n",
        "    def binary_improved_data(self, file_list, num_words=1000):\n",
        "        # Put your code here for Q1b\n",
        "        # Make sure you update the variable features and targets below\n",
        "        # Calculate word count in corpus\n",
        "        news_cnt = corpus_count_words_V2(file_list)\n",
        "    \n",
        "        # Select the most common numWords\n",
        "        word_list = [word for (word, freq) in news_cnt.most_common(num_words)]\n",
        "    \n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        df_rows = []\n",
        "        for file_path in file_list:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                file_data = file.read()\n",
        "                file_data = clean_file_text(file_data)\n",
        "                # Use several techniques to improve the text analyzing performance\n",
        "                NewAnalyzer = RegexTokenizer(r'\\w+') | LowercaseFilter()| StopFilter() | StemFilter()| CustomFilter(WordNetLemmatizer().lemmatize)\n",
        "                file_words = [token.text for token in NewAnalyzer(file_data)]\n",
        "                df_rows.append([1 if word in file_words else 0 for word in word_list])\n",
        "        X = pd.DataFrame(df_rows, columns = word_list)\n",
        "        y = [get_target(get_topic_name(file_path)) for file_path in file_list]\n",
        "\n",
        "        # Please remember to put index for your dataframe as the file name\n",
        "        # For example: pd.DataFrame(data, index=[str(f) for f in file_list],columns=[])\n",
        "\n",
        "        # validate return types\n",
        "        assert isinstance(X, pd.DataFrame) and isinstance(y, list), \"return types\"\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def train_and_predict_baseline(self, file_list):\n",
        "        # get the baseline data\n",
        "        X, y = self.binary_baseline_data(file_list)\n",
        "\n",
        "        # split to train and test set\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        # train a logistic regression classifier\n",
        "        clf = LogisticRegression(C=1.0).fit(X_train, y_train)\n",
        "\n",
        "        # predict on train and test set\n",
        "        y_train_predict = clf.predict(X_train)\n",
        "        y_test_predict = clf.predict(X_test)\n",
        "\n",
        "        # calculate train and test accuracy\n",
        "        train_accuracy = accuracy_score(y_train, y_train_predict)\n",
        "        test_accuracy = accuracy_score(y_test, y_test_predict)\n",
        "\n",
        "        return X, y, train_accuracy, test_accuracy\n",
        "\n",
        "    def train_and_predict_improved(self, file_list):\n",
        "        # get the baseline data\n",
        "        X, y = self.binary_improved_data(file_list)\n",
        "\n",
        "        # Write your code here for Q1c:\n",
        "        # You need to split the data and train a logistic regression classifier.\n",
        "        # Then, you need to calculate the variables train_accuracy and test_accuracy for the new classifier\n",
        "                # split to train and test set\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        # train a logistic regression classifier\n",
        "        clf = LogisticRegression(C=1.0).fit(X_train, y_train)\n",
        "\n",
        "        # predict on train and test set\n",
        "        y_train_predict = clf.predict(X_train)\n",
        "        y_test_predict = clf.predict(X_test)\n",
        "\n",
        "        # calculate train and test accuracy\n",
        "        train_accuracy = accuracy_score(y_train, y_train_predict)\n",
        "        test_accuracy = accuracy_score(y_test, y_test_predict)\n",
        "\n",
        "        return X, y, train_accuracy, test_accuracy\n",
        "\n",
        "    @staticmethod\n",
        "    def random_mean_ci(X, y, num_tests):\n",
        "        # train_results is a list of train accuracy results for the differrent random splits of the dataset\n",
        "        train_results = []\n",
        "\n",
        "        # test_results is a list of test accuracy results for the differrent random splits of the dataset\n",
        "        test_results = []\n",
        "\n",
        "        # Write your code here for Q1d\n",
        "        for i in range(num_tests):\n",
        "          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random.randint(1,1000))\n",
        "      \n",
        "          clf = LogisticRegression(C=1.0).fit(X_train,y_train)\n",
        "\n",
        "          y_train_predict = clf.predict(X_train)\n",
        "          y_test_predict = clf.predict(X_test)\n",
        "\n",
        "          train_accuracy = accuracy_score(y_train,y_train_predict)\n",
        "          test_accuracy = accuracy_score(y_test,y_test_predict)\n",
        "\n",
        "          train_results.append(train_accuracy)\n",
        "          test_results.append(test_accuracy)\n",
        "        # calculate the train mean and the 95% confidence interval for the list of results\n",
        "        train_mean = np.mean(train_results)\n",
        "        train_ci_low, train_ci_high = stats.t.interval(0.95, len(train_results)-1, loc=train_mean, scale=stats.sem(train_results))\n",
        "\n",
        "        # calculate the test mean and the 95% confidence interval for the list of results\n",
        "        test_mean = np.mean(test_results)\n",
        "        test_ci_low, test_ci_high = stats.t.interval(0.95, len(test_results)-1, loc=test_mean, scale=stats.sem(test_results))\n",
        "\n",
        "        # validate return types\n",
        "        assert isinstance(train_mean, float) and isinstance(train_ci_low, float) and isinstance(train_ci_high, float), \"return types\"\n",
        "        assert isinstance(test_mean, float) and isinstance(test_ci_low, float) and isinstance(test_ci_high, float), \"return types\"\n",
        "\n",
        "        return train_mean, train_ci_low, train_ci_high, test_mean, test_ci_low, test_ci_high\n",
        "\n",
        "    @staticmethod\n",
        "    def random_cm(X, y, num_tests):\n",
        "        # cm_list is a list of confusion matrices for the different random splits of the dataset\n",
        "        cm_list = []\n",
        "\n",
        "        # Write your code here for Q1f\n",
        "        for i in range(0,num_tests):\n",
        "          X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.3,random_state = random.randint(1,1000))\n",
        "          clf = LogisticRegression(C=1.0).fit(X_train,y_train)\n",
        "          y_test_predict = clf.predict(X_test)\n",
        "          cm = confusion_matrix(y_test,y_test_predict)\n",
        "          cm_list.append(cm)\n",
        "        # sum the confusion matrices and return the combined confusion matrix\n",
        "        combined_cm = np.array(cm_list).sum(axis=0)\n",
        "\n",
        "        # validate return type\n",
        "        assert isinstance(combined_cm, np.ndarray), \"return type\"\n",
        "\n",
        "        return combined_cm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fkxNi2IWofkC"
      },
      "source": [
        "Please do not change this global variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwVZblX1jJW9"
      },
      "outputs": [],
      "source": [
        "all_files = [pth for pth in Path(DATA_DIR).glob(\"**/*\") if pth.is_file() and not pth.name.startswith(\".\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV2I4MIDjJXF",
        "outputId": "ce4b98ed-54cd-4f6b-8be3-d98e3ae976bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train accuracy: 0.9432021147388726\n",
            "Test accuracy: 0.6865\n"
          ]
        }
      ],
      "source": [
        "q1 = MLQ1()\n",
        "X, y, train_accuracy, test_accuracy = q1.train_and_predict_baseline(all_files)\n",
        "# report results\n",
        "print(\"Train accuracy: {}\".format(train_accuracy))\n",
        "print(\"Test accuracy: {}\".format(test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZSQINnXjJXJ"
      },
      "source": [
        "### Q1 (a)\n",
        "\n",
        "[The feature set consists of the 1000 most commonly occurring words in the file, and each file is represented by a binary value of 1 or 0 for each feature. If a word appears in a file, its corresponding feature value is 1, otherwise it is 0. The dataset contains approximately 19997 samples, which are the number of files in the dataset. The hyperparameter C is set to 1, which is the inverse of the regularization strength. Regularization is a technique that penalizes large parameter values to prevent overfitting. In this case, a small value of C (C=1) is used, which increases the strength of regularization and helps to prevent overfitting.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "qUv9iAy8r6vo",
        "outputId": "3344badc-e2aa-47ec-bef7-53e4f5ce334a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-27b4dce0-7cac-41da-b91c-583204d5204e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>the</th>\n",
              "      <th>to</th>\n",
              "      <th>of</th>\n",
              "      <th>a</th>\n",
              "      <th>and</th>\n",
              "      <th>I</th>\n",
              "      <th>is</th>\n",
              "      <th>in</th>\n",
              "      <th>that</th>\n",
              "      <th>AX</th>\n",
              "      <th>...</th>\n",
              "      <th>friend</th>\n",
              "      <th>HP</th>\n",
              "      <th>isc</th>\n",
              "      <th>present</th>\n",
              "      <th>shall</th>\n",
              "      <th>outside</th>\n",
              "      <th>cars</th>\n",
              "      <th>weapons</th>\n",
              "      <th>recently</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20_newsgroups/alt.atheism/51212</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_newsgroups/alt.atheism/53460</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_newsgroups/alt.atheism/53096</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_newsgroups/alt.atheism/53338</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_newsgroups/alt.atheism/53381</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_newsgroups/sci.crypt/15194</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_newsgroups/sci.crypt/15885</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_newsgroups/sci.crypt/15407</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_newsgroups/sci.crypt/15443</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_newsgroups/sci.crypt/15423</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19997 rows Ã— 1000 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27b4dce0-7cac-41da-b91c-583204d5204e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-27b4dce0-7cac-41da-b91c-583204d5204e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-27b4dce0-7cac-41da-b91c-583204d5204e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                 the  to  of  a  and  I  is  in  that  AX  \\\n",
              "20_newsgroups/alt.atheism/51212    1   1   1  1    1  1   1   1     1   0   \n",
              "20_newsgroups/alt.atheism/53460    1   1   1  0    1  1   1   1     1   0   \n",
              "20_newsgroups/alt.atheism/53096    1   1   1  1    1  1   1   1     1   0   \n",
              "20_newsgroups/alt.atheism/53338    1   1   1  1    1  1   1   1     1   0   \n",
              "20_newsgroups/alt.atheism/53381    1   1   1  1    0  1   1   1     0   0   \n",
              "...                              ...  ..  .. ..  ... ..  ..  ..   ...  ..   \n",
              "20_newsgroups/sci.crypt/15194      1   1   1  0    1  1   0   1     1   0   \n",
              "20_newsgroups/sci.crypt/15885      1   1   1  1    1  1   1   1     1   0   \n",
              "20_newsgroups/sci.crypt/15407      1   1   1  1    1  1   1   1     1   0   \n",
              "20_newsgroups/sci.crypt/15443      1   1   1  1    1  1   1   1     1   0   \n",
              "20_newsgroups/sci.crypt/15423      1   1   1  1    1  1   1   0     1   0   \n",
              "\n",
              "                                 ...  friend  HP  isc  present  shall  \\\n",
              "20_newsgroups/alt.atheism/51212  ...       0   0    0        0      0   \n",
              "20_newsgroups/alt.atheism/53460  ...       0   0    0        0      0   \n",
              "20_newsgroups/alt.atheism/53096  ...       0   0    1        0      0   \n",
              "20_newsgroups/alt.atheism/53338  ...       0   0    0        0      0   \n",
              "20_newsgroups/alt.atheism/53381  ...       0   0    0        0      0   \n",
              "...                              ...     ...  ..  ...      ...    ...   \n",
              "20_newsgroups/sci.crypt/15194    ...       0   0    0        0      0   \n",
              "20_newsgroups/sci.crypt/15885    ...       0   0    0        0      0   \n",
              "20_newsgroups/sci.crypt/15407    ...       0   0    0        0      0   \n",
              "20_newsgroups/sci.crypt/15443    ...       0   0    0        0      0   \n",
              "20_newsgroups/sci.crypt/15423    ...       0   0    0        0      0   \n",
              "\n",
              "                                 outside  cars  weapons  recently  Summary  \n",
              "20_newsgroups/alt.atheism/51212        0     0        0         0        0  \n",
              "20_newsgroups/alt.atheism/53460        0     0        0         0        0  \n",
              "20_newsgroups/alt.atheism/53096        0     0        0         0        0  \n",
              "20_newsgroups/alt.atheism/53338        0     0        0         0        0  \n",
              "20_newsgroups/alt.atheism/53381        0     0        0         0        0  \n",
              "...                                  ...   ...      ...       ...      ...  \n",
              "20_newsgroups/sci.crypt/15194          0     0        0         0        0  \n",
              "20_newsgroups/sci.crypt/15885          0     0        0         0        0  \n",
              "20_newsgroups/sci.crypt/15407          0     0        0         0        0  \n",
              "20_newsgroups/sci.crypt/15443          0     0        0         0        0  \n",
              "20_newsgroups/sci.crypt/15423          0     0        0         0        0  \n",
              "\n",
              "[19997 rows x 1000 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XFlNTqFjJXL"
      },
      "source": [
        "### Q1 (b)\n",
        "\n",
        "Implement *binary_improved_data*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "RQDdKJAlbyTe",
        "outputId": "78a0d565-c2de-4d6b-d756-6b1f1507921f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       edu  ax  com  wa  but  thei  line  new  messag  subject  ...  archiv  \\\n",
              "0        1   0    0   0    1     1     1    1       1        1  ...       0   \n",
              "1        1   0    0   1    1     0     1    1       1        1  ...       0   \n",
              "2        1   0    1   1    0     0     1    1       1        1  ...       0   \n",
              "3        1   0    1   0    1     1     1    1       1        1  ...       0   \n",
              "4        1   0    1   0    0     1     1    1       1        1  ...       0   \n",
              "...    ...  ..  ...  ..  ...   ...   ...  ...     ...      ...  ...     ...   \n",
              "19992    0   0    1   0    0     0     1    0       1        1  ...       0   \n",
              "19993    0   0    1   1    1     1     1    1       1        1  ...       0   \n",
              "19994    1   0    1   1    1     1     1    1       1        1  ...       0   \n",
              "19995    1   0    1   1    1     1     1    0       1        1  ...       0   \n",
              "19996    1   0    1   0    1     1     1    0       1        1  ...       0   \n",
              "\n",
              "       abort  launch  worth  bring  du  shall  iastat  technic  famili  \n",
              "0          0       0      0      0   0      0       0        0       0  \n",
              "1          0       0      0      0   0      0       0        0       0  \n",
              "2          0       0      0      0   0      0       0        0       0  \n",
              "3          0       0      0      1   0      0       0        0       0  \n",
              "4          0       0      0      1   0      0       0        0       0  \n",
              "...      ...     ...    ...    ...  ..    ...     ...      ...     ...  \n",
              "19992      0       0      0      0   0      0       0        0       0  \n",
              "19993      0       0      0      1   0      0       0        1       0  \n",
              "19994      0       0      0      0   0      0       0        0       0  \n",
              "19995      0       0      0      0   0      0       0        0       0  \n",
              "19996      0       0      0      0   0      0       0        1       1  \n",
              "\n",
              "[19997 rows x 1000 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fbf0cdda-9f8e-4ddb-afb4-faa9749a29c6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>edu</th>\n",
              "      <th>ax</th>\n",
              "      <th>com</th>\n",
              "      <th>wa</th>\n",
              "      <th>but</th>\n",
              "      <th>thei</th>\n",
              "      <th>line</th>\n",
              "      <th>new</th>\n",
              "      <th>messag</th>\n",
              "      <th>subject</th>\n",
              "      <th>...</th>\n",
              "      <th>archiv</th>\n",
              "      <th>abort</th>\n",
              "      <th>launch</th>\n",
              "      <th>worth</th>\n",
              "      <th>bring</th>\n",
              "      <th>du</th>\n",
              "      <th>shall</th>\n",
              "      <th>iastat</th>\n",
              "      <th>technic</th>\n",
              "      <th>famili</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19992</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19993</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19994</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19997 rows Ã— 1000 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fbf0cdda-9f8e-4ddb-afb4-faa9749a29c6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fbf0cdda-9f8e-4ddb-afb4-faa9749a29c6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fbf0cdda-9f8e-4ddb-afb4-faa9749a29c6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "q1 = MLQ1()\n",
        "X, y = q1.binary_improved_data(all_files)\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXRUw2LDjJXQ"
      },
      "source": [
        "### Q1 (c)\n",
        "\n",
        "Implement *train_and_predict_improved*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2wliJNvjJXR",
        "outputId": "91dbea55-de29-4c3c-9968-5869ccf5f8ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train accuracy: 0.9609916410659427\n",
            "Test accuracy: 0.7248333333333333\n"
          ]
        }
      ],
      "source": [
        "X, y, train_accuracy, test_accuracy = q1.train_and_predict_improved(all_files)\n",
        "# report results\n",
        "print(\"Train accuracy: {}\".format(train_accuracy))\n",
        "print(\"Test accuracy: {}\".format(test_accuracy))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SST3B968jJXX"
      },
      "source": [
        "[The results indicate an enhancement in both the training and testing tests. The improvement is attributed to the implementation of lemmatization, stemming, and stop words techniques in the analyzer. These techniques effectively eliminate irrelevant words and prioritize meaningful words.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2EbQVhMjJXY"
      },
      "source": [
        "### Q1 (d)\n",
        "\n",
        "Implement *random_mean_ci*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrNbYeqqjJXc"
      },
      "source": [
        "### Q1 (e)\n",
        "\n",
        "Use the following code to calculate the mean accuracy and 95% confidence interval over 10 random splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oerfu1GljJXd",
        "outputId": "ae4d0d55-3112-4608-99a1-253c8bf2d3cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mean accuracy over 10 random splits: 0.9619989997856683\n",
            "Train confidence interval over 10 random splits: [0.960958703793811, 0.9630392957775256]\n",
            "Test mean accuracy over 10 random splits: 0.7228666666666667\n",
            "Test confidence interval over 10 random splits: [0.719753669048378, 0.7259796642849553]\n"
          ]
        }
      ],
      "source": [
        "train_mean10, train_low10, train_high10, test_mean10, test_low10, test_high10 = MLQ1.random_mean_ci(X, y, num_tests = 10)\n",
        "print(\"Train mean accuracy over 10 random splits: {}\".format(train_mean10))\n",
        "print(\"Train confidence interval over 10 random splits: [{}, {}]\".format(train_low10, train_high10))\n",
        "print(\"Test mean accuracy over 10 random splits: {}\".format(test_mean10))\n",
        "print(\"Test confidence interval over 10 random splits: [{}, {}]\".format(test_low10, test_high10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWqqzPkGjJXh"
      },
      "source": [
        "[ The results presented above indicate that the model's performance has minimal variance for both the training and testing datasets, even when using different data splits. This approach is preferable to running a single trial as it provides the user with an understanding of the model's performance variability.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z86lxUvmjJXi"
      },
      "source": [
        "### Q1 (f)\n",
        "\n",
        "Implement *random_cm*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e19w779ejJXs"
      },
      "source": [
        "### Q1 (g)\n",
        "\n",
        "Use the following code to produce a confusion matrix for 10 random splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "xXGSnr-xjJXt",
        "outputId": "b08cdb38-98de-4bae-dfcc-182d39590be6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm10 = MLQ1.random_cm(X, y, num_tests = 10)\n",
        "plot_confusion_matrix(cm10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "yjLh76Qn5moD",
        "outputId": "ec959b9f-1322-423e-ae93-9e69a44a179b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d7746b72-bcfe-4294-9a1c-3582f9b54af3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2478</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>45</td>\n",
              "      <td>20</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>24</td>\n",
              "      <td>244</td>\n",
              "      <td>6</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>19</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>2270</td>\n",
              "      <td>56</td>\n",
              "      <td>26</td>\n",
              "      <td>56</td>\n",
              "      <td>18</td>\n",
              "      <td>14</td>\n",
              "      <td>8</td>\n",
              "      <td>54</td>\n",
              "      <td>28</td>\n",
              "      <td>124</td>\n",
              "      <td>24</td>\n",
              "      <td>34</td>\n",
              "      <td>56</td>\n",
              "      <td>121</td>\n",
              "      <td>42</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>81</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>57</td>\n",
              "      <td>1988</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>72</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>42</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>67</td>\n",
              "      <td>116</td>\n",
              "      <td>300</td>\n",
              "      <td>135</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>113</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32</td>\n",
              "      <td>20</td>\n",
              "      <td>12</td>\n",
              "      <td>1930</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>45</td>\n",
              "      <td>62</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>28</td>\n",
              "      <td>32</td>\n",
              "      <td>9</td>\n",
              "      <td>2464</td>\n",
              "      <td>15</td>\n",
              "      <td>218</td>\n",
              "      <td>9</td>\n",
              "      <td>42</td>\n",
              "      <td>28</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "      <td>10</td>\n",
              "      <td>33</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>68</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>1965</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>281</td>\n",
              "      <td>198</td>\n",
              "      <td>275</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>21</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>210</td>\n",
              "      <td>2</td>\n",
              "      <td>2553</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>19</td>\n",
              "      <td>17</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>22</td>\n",
              "      <td>9</td>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>2571</td>\n",
              "      <td>42</td>\n",
              "      <td>53</td>\n",
              "      <td>12</td>\n",
              "      <td>38</td>\n",
              "      <td>48</td>\n",
              "      <td>11</td>\n",
              "      <td>86</td>\n",
              "      <td>59</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>23</td>\n",
              "      <td>69</td>\n",
              "      <td>36</td>\n",
              "      <td>32</td>\n",
              "      <td>31</td>\n",
              "      <td>22</td>\n",
              "      <td>7</td>\n",
              "      <td>21</td>\n",
              "      <td>2216</td>\n",
              "      <td>78</td>\n",
              "      <td>45</td>\n",
              "      <td>33</td>\n",
              "      <td>91</td>\n",
              "      <td>26</td>\n",
              "      <td>71</td>\n",
              "      <td>34</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>194</td>\n",
              "      <td>32</td>\n",
              "      <td>11</td>\n",
              "      <td>39</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>24</td>\n",
              "      <td>79</td>\n",
              "      <td>1690</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>271</td>\n",
              "      <td>46</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>123</td>\n",
              "      <td>30</td>\n",
              "      <td>28</td>\n",
              "      <td>17</td>\n",
              "      <td>15</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>43</td>\n",
              "      <td>27</td>\n",
              "      <td>2496</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>28</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>74</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>21</td>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>231</td>\n",
              "      <td>7</td>\n",
              "      <td>28</td>\n",
              "      <td>37</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2090</td>\n",
              "      <td>285</td>\n",
              "      <td>95</td>\n",
              "      <td>84</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>10</td>\n",
              "      <td>26</td>\n",
              "      <td>101</td>\n",
              "      <td>28</td>\n",
              "      <td>14</td>\n",
              "      <td>219</td>\n",
              "      <td>13</td>\n",
              "      <td>26</td>\n",
              "      <td>69</td>\n",
              "      <td>16</td>\n",
              "      <td>23</td>\n",
              "      <td>244</td>\n",
              "      <td>1864</td>\n",
              "      <td>149</td>\n",
              "      <td>118</td>\n",
              "      <td>2</td>\n",
              "      <td>55</td>\n",
              "      <td>0</td>\n",
              "      <td>51</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>39</td>\n",
              "      <td>290</td>\n",
              "      <td>16</td>\n",
              "      <td>10</td>\n",
              "      <td>238</td>\n",
              "      <td>7</td>\n",
              "      <td>15</td>\n",
              "      <td>43</td>\n",
              "      <td>14</td>\n",
              "      <td>12</td>\n",
              "      <td>96</td>\n",
              "      <td>153</td>\n",
              "      <td>1697</td>\n",
              "      <td>238</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>107</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4</td>\n",
              "      <td>140</td>\n",
              "      <td>127</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>49</td>\n",
              "      <td>26</td>\n",
              "      <td>42</td>\n",
              "      <td>77</td>\n",
              "      <td>22</td>\n",
              "      <td>28</td>\n",
              "      <td>80</td>\n",
              "      <td>134</td>\n",
              "      <td>215</td>\n",
              "      <td>1784</td>\n",
              "      <td>14</td>\n",
              "      <td>59</td>\n",
              "      <td>0</td>\n",
              "      <td>89</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>34</td>\n",
              "      <td>46</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>24</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>46</td>\n",
              "      <td>47</td>\n",
              "      <td>356</td>\n",
              "      <td>25</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>36</td>\n",
              "      <td>2163</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>20</td>\n",
              "      <td>27</td>\n",
              "      <td>17</td>\n",
              "      <td>57</td>\n",
              "      <td>11</td>\n",
              "      <td>19</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>82</td>\n",
              "      <td>37</td>\n",
              "      <td>9</td>\n",
              "      <td>43</td>\n",
              "      <td>102</td>\n",
              "      <td>18</td>\n",
              "      <td>84</td>\n",
              "      <td>18</td>\n",
              "      <td>2467</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2995</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>6</td>\n",
              "      <td>79</td>\n",
              "      <td>76</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>42</td>\n",
              "      <td>13</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>10</td>\n",
              "      <td>34</td>\n",
              "      <td>12</td>\n",
              "      <td>54</td>\n",
              "      <td>98</td>\n",
              "      <td>111</td>\n",
              "      <td>5</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>2358</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>43</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "      <td>799</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>14</td>\n",
              "      <td>69</td>\n",
              "      <td>421</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>175</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1381</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7746b72-bcfe-4294-9a1c-3582f9b54af3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d7746b72-bcfe-4294-9a1c-3582f9b54af3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d7746b72-bcfe-4294-9a1c-3582f9b54af3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      0     1     2     3     4     5     6     7     8     9     10    11  \\\n",
              "0   2478    10     5    45    20     7     6     6    24   244     6    13   \n",
              "1      7  2270    56    26    56    18    14     8    54    28   124    24   \n",
              "2      2    57  1988     3    12    72     3    10    42     4    11    67   \n",
              "3     32    20    12  1930    21     1     7    10    45    62    11     6   \n",
              "4      4    28    32     9  2464    15   218     9    42    28    14    10   \n",
              "5      1    15    68     4    10  1965    10    19    33     0     9   281   \n",
              "6      7    21    11     2   210     2  2553     1    18    16     3     9   \n",
              "7      4    11     7    22     9    19     3  2571    42    53    12    38   \n",
              "8     23    69    36    32    31    22     7    21  2216    78    45    33   \n",
              "9    194    32    11    39    24     2    14    24    79  1690    20     2   \n",
              "10     4   123    30    28    17    15     8     8    43    27  2496    14   \n",
              "11    11    21    42     3     3   231     7    28    37     3     5  2090   \n",
              "12    10    26   101    28    14   219    13    26    69    16    23   244   \n",
              "13     5    39   290    16    10   238     7    15    43    14    12    96   \n",
              "14     4   140   127     8    36    49    26    42    77    22    28    80   \n",
              "15    34    46     5    26    24     5     7    46    47   356    25    13   \n",
              "16    20    27    17    57    11    19    10    20    82    37     9    43   \n",
              "17     0     0     0     2     0     1     0     0     0     0     0     2   \n",
              "18     6    79    76     3    17    42    13     6    23    10    34    12   \n",
              "19    43    21     6   799     9     4     5    14    69   421    11     4   \n",
              "\n",
              "      12    13    14    15    16    17    18    19  \n",
              "0     12     3     8    19    12     0     2    40  \n",
              "1     34    56   121    42    28     0    81    21  \n",
              "2    116   300   135     3     8     0   113     4  \n",
              "3     21     4     1    23    26     0     1   728  \n",
              "4     13    10    33    11    20     0    30    10  \n",
              "5    198   275    42     8    19     0    31    10  \n",
              "6      8    13    19    17     8     0    18    10  \n",
              "7     48    11    86    59    21     0     8    30  \n",
              "8     91    26    71    34    45     0    23    51  \n",
              "9      4     4    21   271    46     4    15   453  \n",
              "10    14    28    30    15    23     0    74    13  \n",
              "11   285    95    84     4    24     0    23     6  \n",
              "12  1864   149   118     2    55     0    51    14  \n",
              "13   153  1697   238     2     9     1   107     6  \n",
              "14   134   215  1784    14    59     0    89    13  \n",
              "15     3    21    36  2163    17     0     6   196  \n",
              "16   102    18    84    18  2467     0    13    18  \n",
              "17     2     0     0     0     0  2995     3     1  \n",
              "18    54    98   111     5    17     0  2358    12  \n",
              "19    19     2    16   175    24     3     5  1381  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the actual matrix\n",
        "pd.DataFrame(cm10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXgq9z3KjJXw"
      },
      "source": [
        "[The matrix above reveals that there is a confusion between topic 3 and topic 19. Specifically, for the element (3,19), the value is 799, indicating that the model has predicted 799 instances as topic 3 , when in reality, they belong to topic 19 . Similarly, for the element (19,3), the model has predicted 728 instances as topic 19, when they should have been classified as topic 3. This suggests that the two classes are closely related, making it more challenging to predict correctly. Additionally, since both classes are related to \"Talk\" and are closely intertwined, this \n",
        "leads to further confusion. ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTFCjNAjjJXx"
      },
      "source": [
        "## Q2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OAi_6hSofkF"
      },
      "outputs": [],
      "source": [
        "class MLQ2():\n",
        "    def feature_num(self, X, y):\n",
        "        # result_list is a list of tuples (num_features, train_accuracy, test_accuracy)\n",
        "        # where numFeatures is the number of words used as features\n",
        "        result_list = []\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        for p in [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
        "            subset_size = int(p*X.shape[1])\n",
        "            X_train_subset = X_train.iloc[:, 0:subset_size]\n",
        "            X_test_subset = X_test.iloc[:, 0:subset_size]\n",
        "            # Write your code here to calculate train_accuracy and test_accuracy for the current subset of features\n",
        "            clf = LogisticRegression(C=1.0).fit(X_train_subset,y_train)\n",
        "\n",
        "            y_train_predict = clf.predict(X_train_subset)\n",
        "            y_test_predict = clf.predict(X_test_subset)\n",
        "            train_accuracy = accuracy_score(y_train,y_train_predict)\n",
        "            test_accuracy = accuracy_score(y_test,y_test_predict)\n",
        "            # add to result_list\n",
        "            result_list.append((p, train_accuracy, test_accuracy))\n",
        "\n",
        "        # Make a dataframe of the results\n",
        "        result_df = pd.DataFrame(result_list, columns=[\"num_features\", \"train_accuracy\", \"test_accuracy\"])\n",
        "\n",
        "        # validate return type\n",
        "        assert isinstance(result_df, pd.DataFrame), \"return type\"\n",
        "\n",
        "        return result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhXfPuIpjJXy"
      },
      "source": [
        "### Q2 (a)\n",
        "\n",
        "Implement *feature_num*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7hNvwoljJX6"
      },
      "source": [
        "### Q2 (b)\n",
        "\n",
        "Use the following code to plot the train and test accuracy for the different feature sets sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Ehilq_hyjJX7",
        "outputId": "5406e631-1ff2-4a50-a23c-13897236e037"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='num_features'>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "q2 = MLQ2()\n",
        "feature_num_df = q2.feature_num(X, y)\n",
        "feature_num_df.plot(x=\"num_features\", y=[\"train_accuracy\", \"test_accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFk_ess_jJYC"
      },
      "source": [
        "[ The performance score increases as the number of features increases.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0ZftHo6jJYD"
      },
      "source": [
        "## Q3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dl6rebgofkG"
      },
      "outputs": [],
      "source": [
        "class MLQ3():\n",
        "    def hyperparameter(self, X, y):\n",
        "        # result_list is a list of tuples (num_features, train_accuracy, test_accuracy)\n",
        "        # where numFeatures is the number of words used as features\n",
        "        result_list = []\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        for param in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
        "            # Write your code here to calculate train_accuracy and test_accuracy for the current parameter value\n",
        "            clf = LogisticRegression(C=param).fit(X_train,y_train)\n",
        "            y_train_predict = clf.predict(X_train)\n",
        "            y_test_predict = clf.predict(X_test)\n",
        "        \n",
        "            train_accuracy = accuracy_score(y_train,y_train_predict)\n",
        "            test_accuracy = accuracy_score(y_test,y_test_predict)\n",
        "            # add to result_list\n",
        "            result_list.append((param, train_accuracy, test_accuracy))\n",
        "\n",
        "        # Make a dataframe of the results\n",
        "        result_df = pd.DataFrame(result_list, columns=[\"param\", \"train_accuracy\", \"test_accuracy\"])\n",
        "\n",
        "        # validate return type\n",
        "        assert isinstance(result_df, pd.DataFrame), \"return type\"\n",
        "\n",
        "        return result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0e1QSDsjJYE"
      },
      "source": [
        "### Q3 (a)\n",
        "\n",
        "Implement *hyperparameter*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id6-n2ZBjJYM"
      },
      "source": [
        "### Q3 (b)\n",
        "\n",
        "Use the following code to plot the train and test accuracy for the different the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "phBkJ5mvjJYN",
        "outputId": "3d808b49-1d2d-4975-81d5-db7921f2fb96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='param'>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAA2OklEQVR4nO3dd3xUVdrA8d+TRhIgIZDQEkrovRmKIBZcihVFUUTYBQVsuCuWXVxx7Svv6tp2FQUFlyYgKqCiAousBQUChioQqiT0FggQUua8f9xJMoRAJmQmd2byfD/MJ3dufQ4z88yZc889V4wxKKWUClxBdgeglFLKuzTRK6VUgNNEr5RSAU4TvVJKBThN9EopFeA00SulVIALsTuAomJjY03Dhg3tDkMppfzK6tWrDxtj4opb5nOJvmHDhiQnJ9sdhlJK+RUR2X2hZdp0o5RSAU4TvVJKBThN9EopFeB8ro2+ODk5OaSlpZGVlWV3KMpN4eHhJCQkEBoaancoSlV4fpHo09LSqFq1Kg0bNkRE7A5HlcAYw5EjR0hLSyMxMdHucJSq8Pyi6SYrK4saNWpokvcTIkKNGjX0F5hSPsIvavSAJnk/o6+X8iaHw5DjcJCbZ8jNMxgMDmP9mjSAMWAw1l+XaYex/uKyznnbFdmHw1G4ff52joJ1XP4aztmHw7m981/BsYvdzjk/KjyEro1qePz/y28SvVLK8/Ichpw8Bzl5VtLMyXOQ4zDk5DrIdTjIzjXkOhzOdUzBetku22S7butcLzd/umBfzm1cpnNd9nnO8fOP4zBk5zoKpnNyHeQ4rOV5jsC8j0aHetWY91APj+9XE72bjh8/zsyZM3nwwQdLtd3111/PzJkzqVatmncCU6oE2bkOUg+eZGP6CTbszWDj3hNs2X+S09m5eDtfhgUHERIshAYHEer8W/A8KIjQkMLpSqFBVK4Uct66rvs4d3/WesFBQQQJCNYvSTlv2vob5JzGZXmQcM46IuJcdu52ULjfoPz9XmS7IMF5nPx9uMZl7S+oyHZBIkSEBXvlddBE76bjx4/zzjvvnJfoc3NzCQm58H/jwoULvR1amZQUv/IvZ7Lz+HX/CTamWwl9w94Mtu7PJDvPAUDlsGBa141mQKd4osJDi02mhUk2iLBgISQoiNCQwvmhwUGEBAlhIdZf16Qb4kzGVgIWbcLzEX73CX/u841s2nvCo/tsVTeKZ25qfdF1xo4dy/bt2+nQoQOhoaGEh4cTExPD5s2b2bp1K7fccgt79uwhKyuLP/3pT4waNQooHNIhMzOT6667jiuuuILly5cTHx/P/PnziYiIKPZ4kyZNYuLEiWRnZ9OkSROmTZtGZGQkBw4c4P7772fHjh0ATJgwge7duzN16lReffVVRIR27doxbdo0hg0bxo033sjtt98OQJUqVcjMzGTZsmU8/fTTbsX/9ddf89e//pW8vDxiY2NZvHgxzZs3Z/ny5cTFxeFwOGjWrBk//fQTcXHFDrOhvOREVg4b00+w0VlL35CewfZDmQW19JjIUNrERzP8ioa0qRtNm/hoGlSPJChIk29F43eJ3i7jx49nw4YNpKSksGzZMm644QY2bNhQ0H1w8uTJVK9enTNnztC5c2duu+02atQ496RKamoqH330EZMmTeKOO+7gk08+YciQIcUeb8CAAYwcORKAcePG8cEHH/Dwww/zxz/+kauuuorPPvuMvLw8MjMz2bhxIy+++CLLly8nNjaWo0ePllieNWvWlBi/w+Fg5MiRfPfddyQmJnL06FGCgoIYMmQIM2bM4JFHHmHJkiW0b99ek7yXHc48W5DMNzlr6ruPnC5YXjsqnNZ1o7iubR3a1I2idXw0daPDtUatAD9M9CXVvMtLly5dzukj/tZbb/HZZ58BsGfPHlJTU89L9ImJiXTo0AGAyy67jF27dl1w/xs2bGDcuHEcP36czMxM+vbtC8DSpUuZOnUqAMHBwURHRzN16lQGDhxIbGwsANWrV/dI/IcOHeLKK68sWC9/v/fccw/9+/fnkUceYfLkyQwfPrzE4yn3GGPYl5HFBmfTy8a9GWxIP8H+E4VdVetXj6RNfBR3JNWjdd0oWteNJq5qJRujVr7O7xK9r6hcuXLB9LJly1iyZAk//fQTkZGRXH311cX2Ia9UqfDDGBwczJkzZy64/2HDhjFv3jzat2/Phx9+yLJly0odY0hICA6H1TbrcDjIzs4uU/z56tWrR61atVi6dCkrV65kxowZpY5NWV0Edx89XZDMrb8ZHDudA1gnARvFVaFbo+q0iY+mdd1oWtWNIjpCrzZWpaOJ3k1Vq1bl5MmTxS7LyMggJiaGyMhINm/ezM8//1zm4508eZI6deqQk5PDjBkziI+PB+Daa69lwoQJPPLIIwVNN7169eLWW2/l0UcfpUaNGhw9epTq1avTsGFDVq9ezR133MGCBQvIyckpVfzdunXjwQcfZOfOnQVNN/m1+hEjRjBkyBCGDh1KcLB3egoEktw8B9sPnWJDekZBz5dNe0+QeTYXgNBgoVmtqvRpVZvW8VYtvWWdqkSG6UdUlZ2+i9xUo0YNevToQZs2bYiIiKBWrVoFy/r168e7775Ly5Ytad68Od26dSvz8V544QW6du1KXFwcXbt2LfiSefPNNxk1ahQffPABwcHBTJgwgcsvv5ynnnqKq666iuDgYDp27MiHH37IyJEj6d+/P+3bt6dfv37n1OJdXSj+uLg4Jk6cyIABA3A4HNSsWZPFixcDcPPNNzN8+HBttilGVk4eWw+cLKyl7z3B5n0nOJtr/boKDw2iVZ0obu0YTxtnUm9WqyphIX5xobryQ2KMb114kJSUZIreeOTXX3+lZcuWNkWkipOcnMyYMWP4/vvvL7hORXjdTp3N5dd9J5w1devvtoOZ5Dq7vlQND6F13aiCXi+t60bRKK4KwdrzRXmYiKw2xiQVt0xr9KrUxo8fz4QJEypc2/zJrBzWpWUUnCjdsDeDnYdPFVwaX6NyGG3io+nVoiZt4qNpUzeaetUjtOeLsp0meps99NBD/Pjjj+fM+9Of/uTTTSJjx45l7NixdodRbowxfLImnec+38jJLKtNPb5aBK3qRtG/fWHzS62oSprUlU/SRG+zt99+2+4Q1EUcPJHFXz9bz5JfD9K5YQyjezWlbXw01SuH2R2aUm7TRK9UMYwxLFi7l7/N30hWTh7jbmjJ8B6J2rau/JImeqWKOJx5lqc+W883Gw/QsX41Xh3YnsZxVewOS6lL5laiF5F+wJtAMPC+MWZ8keUNgMlAHHAUGGKMSXMuywPWO1f9zRhzs4diV8rjvly3j6fnbyAzK5ex17VgZM9GWotXfq/EjrsiEgy8DVwHtALuEpFWRVZ7FZhqjGkHPA+87LLsjDGmg/Pht0k+f/TKS/HGG29w+vTpkldUtjl6KpuHZq7hoZlrSIiJ4Ms/XsH9VzXWJK8CgjtXaHQBthljdhhjsoFZQP8i67QCljqnvy1mud8LlESfm5trdwg+5+sN++nz+v9YtHE/T/RtzqcPdKdprap2h6WUx7iT6OOBPS7P05zzXK0FBjinbwWqikj+iF7hIpIsIj+LyC3FHUBERjnXST506JD70Zcj12GKn3jiCV555RU6d+5Mu3bteOaZZwA4deoUN9xwA+3bt6dNmzbMnj2bt956i71793LNNddwzTXXXHD/DzzwAElJSbRu3bpgfwCrVq2ie/futG/fni5dunDy5Eny8vJ4/PHHadOmDe3ateNf//oXYA2JfPjwYcC6oOnqq68G4Nlnn2Xo0KH06NGDoUOHsmvXLnr27EmnTp3o1KkTy5cvLzje//3f/9G2bVvat29fUOZOnToVLE9NTT3nuT87fjqbR2b9wv3TV1OzajgLRl/BQ9c0ISRYr1BVgcVTJ2MfB/4tIsOA74B0IM+5rIExJl1EGgFLRWS9MWa768bGmInARLCujL3okb4aC/vXX3SVUqvdFq4bf9FVXIcpXrRoEXPnzmXlypUYY7j55pv57rvvOHToEHXr1uXLL78ErDFkoqOjee211/j2228LRpcszksvvUT16tXJy8vj2muvZd26dbRo0YI777yT2bNn07lzZ06cOEFERAQTJ05k165dpKSkEBIS4tawxJs2beKHH34gIiKC06dPs3jxYsLDw0lNTeWuu+4iOTmZr776ivnz57NixQoiIyMLxraJjo4mJSWFDh06MGXKFJ/u4++u//56gLGfrufYqWwe+V1THrqmCaGa4FWAcifRpwP1XJ4nOOcVMMbsxVmjF5EqwG3GmOPOZenOvztEZBnQETgn0fubRYsWsWjRIjp27AhAZmYmqamp9OzZk8cee4y//OUv3HjjjfTs2dPtfc6ZM4eJEyeSm5vLvn372LRpEyJCnTp16Ny5MwBRUVEALFmyhPvvv7/gzlDuDEt88803F9zkJCcnh9GjR5OSkkJwcDBbt24t2O/w4cOJjIw8Z78jRoxgypQpvPbaa8yePZuVK1e6XS5fk3Emh+c/38Qna9JoUbsqU4Z1pk18tN1hKeVV7iT6VUBTEUnESvCDgMGuK4hILHDUGOMAnsTqgYOIxACnjTFnnev0AP5RpohLqHmXB2MMTz75JPfdd995y9asWcPChQsZN24c1157LX/7299K3N/OnTt59dVXWbVqFTExMQwbNuyiwwRfiOuwxEW3dx3Q7PXXX6dWrVqsXbsWh8NBeHj4Rfd722238dxzz9GrVy8uu+yy88bZ9xfLthxk7CfrOZR5ltHXNOHha5tQKURH3lSBr8TfqsaYXGA08A3wKzDHGLNRRJ4XkfxeNFcDW0RkK1ALeMk5vyWQLCJrsU7SjjfGbPJwGcqF6zDFffv2ZfLkyWRmZgKQnp7OwYMH2bt3L5GRkQwZMoQnnniCNWvWnLdtcU6cOEHlypWJjo7mwIEDfPXVVwA0b96cffv2sWrVKsAaujg3N5fevXvz3nvvFZxYzW+6yR+WGOCTTz654PEyMjKoU6cOQUFBTJs2jbw8q5Wtd+/eTJkypeDEcf5+w8PD6du3Lw888IBfNtuczMph7CfrGDZlFVXCQ/j0ge483re5JnlVYbjVRm+MWQgsLDLvby7Tc4G5xWy3HGhbxhh9guswxddddx2DBw/m8ssvB6x7sU6fPp1t27bxxBNPEBQURGhoKBMmTABg1KhR9OvXj7p16/Ltt9+et+/27dvTsWNHWrRoQb169ejRowcAYWFhzJ49m4cffpgzZ84QERHBkiVLGDFiBFu3bqVdu3aEhoYycuRIRo8ezTPPPMO9997L008/XXAitjgPPvggt912G1OnTj1n+OJ+/fqRkpJCUlISYWFhXH/99fz9738H4O677+azzz6jT58+nvxv9boftx3mz3PXsS/jDPdd1Ygxv2tGeKgmeFWx6DDFyi2vvvoqGRkZvPDCC25vY+frdupsLi9/9SvTf/6NRrGVeWVgey5rEGNLLEqVBx2mWJXJrbfeyvbt21m6dGnJK/uAn3cc4Ym5a0k7doYRVyTyeN/mWotXFZom+nLWtWtXzp49e868adOm0bat77Zw5d803Nedzs7lH19v4cPlu2hQI5LZoy6nS2LJPZKUCnSa6MvZihUr7A4hICXvOsrjH69l15HTDOvekD/3a673W1XKyW8+CcYYvamDHymvcz9ZOXn8c9EW3v9hJ/HVIvhoZDcub+yf3T+V8ha/SPTh4eEcOXKEGjVqaLL3A8YYjhw5UmL//LL65bdjPPbxWnYcOsXdXevz5PUtqVLJL97SSpUrv/hUJCQkkJaWhq+Og6POFx4eTkJCglf2nZWTxxtLUpn43XZqR4Uz7d4u9Gwa55VjKRUI/CLRh4aGkpiYaHcYygesSzvOY3PWknowkzuT6vHUjS2JCg+1OyylfJpfJHqlsnMd/GtpKu8s205slTCmDO/MNc1r2h2WUn5BE73yeRv3ZvDYnLVs3n+SAZ3ieebG1kRHai1eKXdpolc+KyfPwTvfbudfS1OJqRzGpN8n0btVLbvDUsrvaKJXPmnL/pM89nEKG9JP0L9DXZ69qTUxlcPsDkspv6SJXvmU3DwH7323gzeWbCUqPJR3h3SiX5s6doellF/TRK98xraDJ3lszlrWpmVwfdvavNC/DTWqVLI7LKX8niZ6Zbs8h+H973fwz8VbqRwWzL8Hd+TGdnXtDkupgKGJXtlqx6FMHv94LWt+O06fVrV46da2xFXVWrxSnqSJXtnC4TBMWb6Lf3y9mUohQbxxZwf6d6irQ1wo5QWa6FW5233kFE98vI6Vu47Sq0VNXh7QllpR3h0XR6mKTBO9KjcOh2H6it28vHAzIUHCK7e34/bLErQWr5SXlXhzcAAR6SciW0Rkm4iMLWZ5AxH5r4isE5FlIpLgsuwPIpLqfPzBk8Er/7Hn6Gnufn8Ff5u/kc6J1flmzJUMTKqnSV6pclBijV5EgoG3gd5AGrBKRBYYYza5rPYqMNUY8x8R6QW8DAwVkerAM0ASYIDVzm2PebogyjcZY/ho5R5e+tJ6u7w8oC2DOmuCV6o8udN00wXYZozZASAis4D+gGuibwU86pz+FpjnnO4LLDbGHHVuuxjoB3xU5siVz8vNc/DonLUsWLuX7o1r8I/b25EQE2l3WEpVOO403cQDe1yepznnuVoLDHBO3wpUFZEabm6LiIwSkWQRSdYx5wODw2H489x1LFi7l8f7NGP6vV01yStlE7fa6N3wOHCViPwCXAWkA3nubmyMmWiMSTLGJMXF6Q0k/J0xhqfmrefTX9J5tHczRvdqSlCQNtUoZRd3mm7SgXouzxOc8woYY/birNGLSBXgNmPMcRFJB64usu2yMsSrfJwxhuc+38RHK/fw0DWNebhXE7tDUqrCc6dGvwpoKiKJIhIGDAIWuK4gIrEikr+vJ4HJzulvgD4iEiMiMUAf5zwVgIwxjP9qMx8u38W9VyTyeJ/metJVKR9QYqI3xuQCo7ES9K/AHGPMRhF5XkRudq52NbBFRLYCtYCXnNseBV7A+rJYBTyff2JWBZ7Xl6Ty3nc7GNKtPuNuaKlJXikfIcYYu2M4R1JSkklOTrY7DFVKb3+7jVe+2cIdSQmMH9BO2+SVKmcistoYk1TcMk+djFUV2Ac/7OSVb7bQv0NdXtYkr5TP0USvymTaz7t54YtNXNemNv8c2J5gTfJK+RxN9OqSzUnew9PzNnBti5q8OagjIcH6dlLKF+knU12S+Snp/OWTdfRsGsvbd3ciLETfSkr5Kv10qlL7esM+Hp2zli4NqzNxaBLhocF2h6SUughN9KpUlm4+wMMf/UL7hGg+GNaZiDBN8kr5Ok30ym3fpx7i/ulraFE7ig/v6UKVSno7A6X8gSZ65Zafdxxh5NRkGsVWZuo9XYgKD7U7JKWUmzTRqxKt3n2Mez9cRUJMJNNHdCWmcpjdISmlSkETvbqo9WkZDJu8kriqlZgxoiuxVSrZHZJSqpQ00asL+nXfCYZOXkFURCgzRnbTG3gr5ac00atibTt4kiHvryA8JJiPRnYjvlqE3SEppS6RJnp1nl2HTzF40gpEhBkju1K/ht4ZSil/polenSPt2Gnufn8FOXkOZozoSuO4KnaHpJQqI030qsD+jCwGT1rByawcpt3blea1q9odklLKA/SKFwXAwZNZDJ70M0dPZTN9RFfaxEfbHZJSykO0Rq84eiqbIe+vYF9GFlOGd6ZDvWp2h6SU8iBN9BVcxukchn6wgt1HTvPBH5Lo3LC63SEppTxME30FdjIrh99PWUnqgUzeG3oZ3ZvE2h2SUsoL3Er0ItJPRLaIyDYRGVvM8voi8q2I/CIi60Tkeuf8hiJyRkRSnI93PV0AdWlOZ+dyz4er2Jiewb8Hd+Tq5jXtDkkp5SUlnowVkWDgbaA3kAasEpEFxphNLquNA+YYYyaISCtgIdDQuWy7MaaDR6NWZZKVk8eI/ySzevcx3rqrI31a17Y7JKWUF7lTo+8CbDPG7DDGZAOzgP5F1jFAlHM6GtjruRCVJ53NzeP+6av5accRXh3Ynhvb1bU7JKWUl7mT6OOBPS7P05zzXD0LDBGRNKza/MMuyxKdTTr/E5GeZQlWlU1OnoOHZ/7Csi2H+PutbRnQKcHukJRS5cBTJ2PvAj40xiQA1wPTRCQI2AfUN8Z0BB4FZopIVNGNRWSUiCSLSPKhQ4c8FJJylecwjJmdwqJNB3j2plbc1aW+3SEppcqJO4k+Hajn8jzBOc/VvcAcAGPMT0A4EGuMOWuMOeKcvxrYDjQregBjzERjTJIxJikuLq70pVAX5XAYnpi7li/W7ePJ61owrEei3SEppcqRO4l+FdBURBJFJAwYBCwoss5vwLUAItISK9EfEpE458lcRKQR0BTY4angVcmMMTw1bwOfrknn0d7NuO+qxnaHpJQqZyX2ujHG5IrIaOAbIBiYbIzZKCLPA8nGmAXAY8AkERmDdWJ2mDHGiMiVwPMikgM4gPuNMUe9Vhp1DmMMz32+iY9W/saDVzfm4V5N7A5JKWUDMcbYHcM5kpKSTHJyst1h+D1jDOO/3sx7/9vBvVckMu6GloiI3WEppbxERFYbY5KKW6ZXxgaoN5ak8t7/djCkW31N8kpVcJroA9A7y7bx5n9TGXhZAs/f3EaTvFIVnCb6APPBDzv5x9db6N+hLuNva0dQkCZ5pSo6TfQBZPrPu3nhi030a12bfw5sT7AmeaUUmugDxsfJexg3bwO9WtTkrbs6EhKsL61SyqLZIADMT0nnL5+so2fTWN65uxNhIfqyKqUKaUbwc19v2Mejc9aS1LA6E4cmER4abHdISikfo4nejy3dfICHP/qF9gnRTB7WmYgwTfJKqfNpovdTP6Qe5v7pa2hRO4opw7tQpZLe510pVTxN9H5oxY4jjJi6ikaxlZl6TxeiI0LtDkkp5cM00fuZ1buPcc+Hq4ivFsH0EV2JqRxmd0hKKR+nid6PbEjPYNiUlcRWrcTMkd2IrVLJ7pCUUn5AE72f2Lz/BEM+WEFUeCgzR3ajVlS43SEppfyEJno/sO1gJndPWkF4SDAzR3YlvlqE3SEppfyIJnoft+vwKQZP+hkRYcbIrjSoUdnukJRSfkYTvQ9LO3aau99fQU6egxkjutI4rordISml/JAmeh+1PyOLwZNWcDIrh2n3dqV57ap2h6SU8lN6lY0POpJ5lsHv/8zRU9lMu7cLbeKj7Q5JKeXHNNH7GGMMf567jrRjZ5gxoisd68fYHZJSys9p042P+WjlHv67+SBj+7Wgc8PqdoejlAoAbiV6EeknIltEZJuIjC1meX0R+VZEfhGRdSJyvcuyJ53bbRGRvp4MPtDsOnyKF77YxBVNYhnWvaHd4SilAkSJTTciEgy8DfQG0oBVIrLAGLPJZbVxwBxjzAQRaQUsBBo6pwcBrYG6wBIRaWaMyfN0Qfxdbp6DR2anEBosvDJQbwGolPIcd2r0XYBtxpgdxphsYBbQv8g6BohyTkcDe53T/YFZxpizxpidwDbn/lQR7yzbTsqe47x0a1vqROsFUUopz3En0ccDe1yepznnuXoWGCIiaVi1+YdLsS0iMkpEkkUk+dChQ26GHjjW7jnOm/9N5ZYOdbmpfV27w1FKBRhPnYy9C/jQGJMAXA9MExG3922MmWiMSTLGJMXFxXkoJP9wOjuXMbNTqFW1Es/1b2N3OEqpAORO98p0oJ7L8wTnPFf3Av0AjDE/iUg4EOvmthXayws3s+PwKWaO7KrjyiulvMKdWvcqoKmIJIpIGNbJ1QVF1vkNuBZARFoC4cAh53qDRKSSiCQCTYGVngre33275SDTft7NiCsS6d441u5wlFIBqsQavTEmV0RGA98AwcBkY8xGEXkeSDbGLAAeAyaJyBisE7PDjDEG2Cgic4BNQC7wkPa4sRw9lc2f566jRe2qPN63ud3hKKUCmFj52HckJSWZ5ORku8PwKmMM909fzbebDzF/dA9a1okqeSOllLoIEVltjEkqbpleGWuDuavT+GbjAR7r00yTvFLK6zTRl7M9R0/z3Oeb6JpYnRE9G9kdjlKqAtBEX47yHIZH56QgwD/vaE+wXv2qlCoHOnplOXrvu+2s2nWM1+5oT0JMpN3hKKUqCK3Rl5MN6Rm8vngrN7Stw60dz7s4WCmlvEYTfTnIysljzOwUYiLDePGWNohok41Sqvxo0005+L+vN5N6MJOp93QhpnKY3eEopSoYrdF72Q+ph5ny4y6GdW/Ilc0q1jg+SinfoInei46fzubxj9fSOK4yf+nXwu5wlFIVlCZ6LzHGMG7eBg5nnuXNQR2JCAu2OySlVAWlid5LFqzdyxfr9jGmdzPaxEfbHY5SqgLTRO8F6cfPMG7eBi5rEMN9V+rVr0ope2mi9zCHw/D4nLU4HIbX7+hASLD+Fyul7KVZyMMm/7iTn3Yc4W83taJ+Db36VSllP030HrR5/wn+8fUW+rSqxR1J9UreQCmlyoEmeg85m5vHI7NSiIoI4eUBbfXqV6WUz9ArYz3ktUVb2bz/JJOHJVGjSiW7w1FKqQJao/eAn3ccYeL3OxjctT69WtSyOxyllDqHJvoyOpGVw2Nz1tKgeiTjbmhpdzhKKXUetxK9iPQTkS0isk1Exhaz/HURSXE+torIcZdleS7LFngwdp/w7PyN7D+Rxet3diAyTFvClFK+p8TMJCLBwNtAbyANWCUiC4wxm/LXMcaMcVn/YaCjyy7OGGM6eCxiH/Llun18+ks6f7q2KR3rx9gdjlJKFcudGn0XYJsxZocxJhuYBfS/yPp3AR95Ijhftj8ji79+tp72CdGM7tXE7nB8U84Z2L0ctn8LOVl2R6NUheVOW0M8sMfleRrQtbgVRaQBkAgsdZkdLiLJQC4w3hgz79JC9R0Oh+GJuWvJznXw+p0dCNWrX8EYOLYL0lZZjz0r4cAGcORay0MjIfFKaNrHelTT6wyUKi+eblQeBMw1xuS5zGtgjEkXkUbAUhFZb4zZ7rqRiIwCRgHUr1/fwyF53rSfd/N96mFevKUNjeKq2B2OPc5mwt5fIG0lpCVbyf3UIWtZaGWI7wTd/wj1uoAEQepiSP0Gtn5trRPXEpo5k369rhAcal9ZlApw7iT6dMC1+pXgnFecQcBDrjOMMenOvztEZBlW+/32IutMBCYCJCUlGXcCt8u2gyf5+8JfuaZ5HHd39f0vJY8wBo5sd9bWV1p/D2wE47CW12gCTXpDQpKV2ONaQnCRt1azvmBegcOpVsJPXQQ/vQ0/vgmVoqDxNVbSb9IbqmoXVaU8yZ1EvwpoKiKJWAl+EDC46Eoi0gKIAX5ymRcDnDbGnBWRWKAH8A9PBG6H7FwHj8xOoXKlEP7v9naBe/Vr1gnYuwb2uCT2M8esZWFVIeEy6PkYJHSxkntkdff2KwJxzaxH94et4+z8H2z9xqrxb5pvrVenQ2ETT3wnCNKx/JUqixITvTEmV0RGA98AwcBkY8xGEXkeSDbG5HeZHATMMsa41shbAu+JiAPrxO941946/uat/6ayIf0E7w65jJpVw+0OxzMcDjiSWtiunpYMBzcBzpcxtjm0uAESOluJPa655xJveBS0vMl6GAP711s1/dTF8P2r8N0/ILIGNPmdlfQb93L/S0UpVUDOzcv2S0pKMsnJyXaHcZ7Vu48y8N2fuK1TAq8MbG93OJfuzHFIT7YS+p6V1nRWhrUsPBrinc0vCUnWdEQ1e+I8fRS2Ly1M/GeOWm39CV2gaW8r8ddua/1KUEohIquNMUnFLtNEX7LMs7lc/+b3GAxf/elKqlTykwujHHlwaEth88ueVXB4i3OhQM1Whe3qCZ2hRlMI8sEeRI48SF/jTPqLYF+KNb9qncKk3+hqqFTVziiVstXFEr2fZCx7vfD5JtKOnWb2fZf7dpI/fdTZA8aZ2NNWQ/ZJa1lEdSuZtx0I9TpD3U5W04k/CAq2Yq7XGXo9BScPwLYl1kndjfNgzVQICoUGlzvb9vtCbFOt7SvlpDX6EnyzcT/3TVvNg1c35s/9WtgdTqG8XKstPb/fetoqOLLNWiZBUKu182RpZ6vGXr1RYCa+vBzYs6LwhO6hX6351RoUntBN7AmhEfbGqZSXadPNJTp08ix93/iOOtHhfPZgD8JCbGzWOHXY5YTpKqspI+eUtSwytrD5JaEz1O0IlSpo//7jvzn77C+2evTknIaQcJeLtXpDTEO7o1TK47Tp5hIYY/jLJ+s4dTaXN+7sUL5JPr8Hyp4VhYn92E5rWVCIdRKy492F3RtjGgZmbf1SVKsPne+1HjlZsPsHK+lvdfbdB6snUX7bfv3LISTM3piV8jKt0V/AjBW7eeqzDTxzUyuG90gs34Mv/pt1IRFAldpW23R+98Y67SFM70V7SQ5vKzyhu/tHyMu2rgtofHXhxVpRdeyOUqlLojX6Utp5+BQvfvErVzSJ5Q+XNyzfg//4lpXkO/0ernwCoutpbd1TYptYj8sftIZw2Pmd8yrdxfDr59Y6tdtaJ3Ob9rF+LenFWioAaI2+iNw8B7e/+xM7D5/im0eupHZ0OV4YlTIT5j0ArW6B2ydrkikvxlgntlMXwdZFVpOZyYOIGGh8rTV8Q+NroXINuyNV6oK0Rl8K//52Gyl7jvPvwR3LN8lv+Rrmj4bEq2DARE3y5UnE6qVUqzVcMcYa7mH7t1ZNf9ti2DAXEKuG3/AKq2tq3Y4QnaC/tpRf0ETvImXPcf61dBu3dKjLje3qlt+Bd/8EH//BajYYNANC9ObitoqIgTYDrIfDAft+cfbkWQTL/1U49HLluMKkH9/Jmq4SZ2/sShVDE73T6excxsxOoVbVSjzXv035HfjARvjoTqt2OOQTvbrT1wQFQfxl1uPqsVZPngMbrUHf0tdYf1MXUTA2UFQCxHe0kn58J2uANruGkVDKSRO900tf/squI6eYOaIb0RHlNDb6sd0wbYB1U44hn0Ll2PI5rrp0oeHW6J0JlxXOO5sJ+9ZaSX/vL9YXQP7JXYDqjZ01fucXQJ12EFa5/GNXFZYmeuDbzQeZseI3RvZM5PLG5XTCLfMQTLsVcs/A8K8hpkH5HFd5XqUq0LCH9ch3+qg1Jk+6M/nvXg7rP7aWSRDEtXDW+p3Jv1ZrbbJTXlPhE/2RzLM8MXcdLWpX5fG+zcvnoGdPwozb4cRe+P08qNWqfI6ryk9kdWtY5ca9Cued3G8l/fxa/9avIGW6tSw4zEr2rm3+sc3Pv4GLUpegQr+LjDE8+el6TpzJYdq9XagUUg49XXLPwqy7rStf7/oI6nfz/jGVb6haG5pfZz3A6tZ5/Ddn8ne2+a//GJI/sJaHRkLtdoUneuM7QUyib44wqnxahU70H69OY9GmA/z1+ha0rFMOIzk68uDTkdYYLLe8a/XPVhWXiNVkF9MAWt9izXM44Oj2wiafvWsgeQrkvmMtrxQNdTuc2+av3TxVCSpsov/tyGmeW7CRbo2qM+KKRt4/oDGw8HHrdnl9XoIOd3n/mMr/BAVZQyzHNoX2d1rz8nLh0OZze/qc183TpaePdvNURVTIRJ/nMDw6J4UgEV4d2J6goHKoDS0bD8mToccj0H2094+nAkdwCNRuYz06/d6a59rNM7/NP3UxxXbzrNvRemg3zwqrQib6d/+3neTdx3j9zvYkxJTDAGErJ8H/xkOHIfC7Z71/PBX4LtTNc/+6wlr/3l/O7+ZZcHFXR+seBZVrapt/BVDhEv2G9AxeX7yVG9rV4ZYO8eVwwE9g4RPQ/Hq46U1tS1XeU6kKNOhuPfKdOXZuT5/ffnIO6eAUHAZR8VCtnjWAXnQ953SC83mCdvsMAG4lehHpB7wJBAPvG2PGF1n+OnCN82kkUNMYU8257A/AOOeyF40x//FA3JckKyePR2anUKNKGC/d0gbxdtLdvhQ+vc/qWXP7ZO0qp8pfREwx3TwPWDX/Y7sgYw9kpMHxPdb79eR+Cpp/8lWpVZj0q9WD6Pou0/Wsm8prBcanlZh5RCQYeBvoDaQBq0RkgTFmU/46xpgxLus/DHR0TlcHngGSsN49q53bHvNoKdw0/qvNbDuYybR7u1At0ss3m0hfDbOGQGwzuGuW3spO+Y6qtaBq7+KX5WbDiXQr+Wfssb4AMpyP/ethy1eQd/bcbcKquvwiSDj/10GVWjpIn83cqWJ2AbYZY3YAiMgsoD+w6QLr34WV3AH6AouNMUed2y4G+gEflSXoS/F96iE+XL6LYd0b0rOpl3skHE6F6bdbw9oO/VRPgin/ERIG1ROtR3GMgVOHzv0COO78VZDxmzXEc9bxc7cJCoWoutbdv1ybhAp+HcRrRcjL3En08cAel+dpQNfiVhSRBkAisPQi257XMC4io4BRAPXr13cjpNI5fjqbxz9eS5OaVRh7nZdv8H1irzW0QVAwDJ1nXSSjVKAQgSo1rYfriWBXZ08WNgcV/TLY+R2c3AfGce42leNcvgDqn//rICJGm4fKwNONxoOAucaYvNJsZIyZCEwE68YjngzIGMNT8zZwJDObD/7QmfBQL/6EPH3UGqTszHEY9gXUaOy9YynlqypVhZotrUdx8nKsCpHr+YGM36zpg79ao4HmZp27TWjlc08SuzYPVa1t3QA+pBIEh0Kw869+MRRwJ9GnA/Vcnic45xVnEPBQkW2vLrLtMvfDK7v5KXv5ct0+nujbnDbx0d47UPZpmHmndVXjkE+sqxeVUucLDi28Irg4xsDpI9bwEOd8GTgf6WvgzFE3jhNmJf2QMOd0mPPLwPlFEFKpyPywc78szpkOK7I/130UmT7veEWmbfgCcifRrwKaikgiVuIeBAwuupKItABigJ9cZn8D/F1EYpzP+wBPliniUkg/foan528gqUEM91/lxdp1Xo5145D0ZBj4H0i80nvHUirQiVhDdleOtfr8F+dspnXS+PgeyDxgnSDOzbZu+F4wfdb6bOaeLTLtXC/3rPXLISvDWlbsPrLBkePZ8gUV/ZLJ/+IIs4awvvVdzx4PNxK9MSZXREZjJe1gYLIxZqOIPA8kG2MWOFcdBMwyLjehNcYcFZEXsL4sAJ7PPzHrbQ6H4bE5KTgchtfu6ECwt65+dThg/kPWz80b34BWN3vnOEqpQpWqQFxz6+FtDkdh8i/pi8P1b8F0/pfI2SJfKPnru+w70jvDpLvVRm+MWQgsLDLvb0WeP3uBbScDky8xvkv2wQ87+XnHUf5xWzvq1/DS1a/GwKJxsG42XDMOkoZ75zhKKfsEBUFQuHU1sp8KyGuff913gle+2UKfVrUYmJTgvQP9+Ab8/DZ0uQ+ufNx7x1FKqTIIuESflZPHmNkpREWE8vKAtt67+nXNVFjyLLS5HfqN1zP8SimfFXDX5L+2eCub959k8rAkalTx0hgdm7+Ez/9kXVZ+ywQdFEop5dMCKkP9tP0Ik77fwd1d69OrRS3vHGTXjzD3Hmv0vzumWWfMlVLKhwVMoj+RlcNjc1JoWKMyT91wgQs1ymr/evjoLuvKvcEfW2f+lVLKxwVM001WTh5NalVlzO+aEhnmhWId3QnTb7OS+5BPrXFslFLKDwRMoq9ZNZyp93Txzs4zD1rj1+Rlw+8XWJdfK6WUnwiYRO81WRkwfYB19d3vF0BNLw+KppRSHqaJ/mJysmDW3dZAS3fNhnqd7Y5IKaVKTRP9hTjy4JN7Ydf3MOB9aPo7uyNSSqlLEjC9bjzKGPhiDGz+wroYqt1AuyNSSqlLpom+OEtfhDX/gZ6PQbcH7I5GKaXKRBN9UT+/C9+/Cp1+D72etjsapZQqM030rtZ9DF//BVrcCDe8ruPXKKUCgib6fKlLYN790OAKuO0DCNbz1EqpwKCJHiAtGeYMte5xeddMvx53WimlitJEf2gLzLgdqtSyhjYI9+J9ZZVSygYVO9FnpFlDGwSHwdDPoEpNuyNSSimPq7gN0aePWkn+7EkYvhCqJ9odkVJKeYVbNXoR6SciW0Rkm4iMvcA6d4jIJhHZKCIzXebniUiK87GguG3LXfYpmDEQju2Gu2ZB7bZ2R6SUUl5TYo1eRIKBt4HeQBqwSkQWGGM2uazTFHgS6GGMOSYirm0gZ4wxHTwbdhnkZsPsobB3Ddw5HRr2sDsipZTyKndq9F2AbcaYHcaYbGAW0L/IOiOBt40xxwCMMQc9G6aHOBww/0HY/l+46U1ocYPdESmllNe5k+jjgT0uz9Oc81w1A5qJyI8i8rOI9HNZFi4iyc75t5Qt3DIwBr55EtZ/DNc+Y135qpRSFYCnTsaGAE2Bq4EE4DsRaWuMOQ40MMaki0gjYKmIrDfGbHfdWERGAaMA6tev76GQivj+n7DiXej2EFwxxjvHUEopH+ROjT4dcL2lUoJznqs0YIExJscYsxPYipX4McakO//uAJYBHYsewBgz0RiTZIxJiouLK3UhSrT6Q1j6ArS7E/q8qEMbKKUqFHcS/SqgqYgkikgYMAgo2ntmHlZtHhGJxWrK2SEiMSJSyWV+D2AT5WnTAmvI4Sa9of/bEFSxLx1QSlU8JTbdGGNyRWQ08A0QDEw2xmwUkeeBZGPMAueyPiKyCcgDnjDGHBGR7sB7IuLA+lIZ79pbx+t2fmfdPCQ+Ce74DwSHltuhlVLKV4gxxu4YzpGUlGSSk5PLvqN9a2HKDRAdD8O/gsjqZd+nUkr5KBFZbYxJKm5ZYLZjHNkO02+DiGrW+DWa5JVSFVjgJfqT+2H6ADAOa/ya6KI9QZVSqmIJrLFuzhyH6bdD5iEY9jnENrU7IqWUsl3gJPqcMzBrMBzaDHfPgfjL7I5IKaV8QuA03WQetIYdHvAeNO5ldzRKKeUzAqdGH9MAHlqpd4dSSqkiAqdGD5rklVKqGIGV6JVSSp1HE71SSgU4TfRKKRXgNNErpVSA00SvlFIBThO9UkoFOE30SikV4HxumGIROQTsdpkVDWRc4Hn+tOu8WODwJR6+6LFKs05x8y8Wu+vz4spUlnJcLE531iltWUqatus1udAyfyxLWd5frtP++Fnx5mtysTjdWceXytLAGFP8LfqMMT79ACZe6Hn+dJF5yZ46VmnWKW7+xWK/SPz58y65HOVdlpKm7XpNAqksZXl/XeS95hdl8eZrEmhludDDH5puPr/I888vsI6njlWadYqbf7HYXZ8XV6ayKs+yuDN9qcpSjgst88eylOX95Tqt7y/34nF3HV8rS7F8rummrEQk2VzgLiv+JFDKAVoWXxUoZQmUcoD3yuIPNfrSmmh3AB4SKOUALYuvCpSyBEo5wEtlCbgavVJKqXMFYo1eKaWUC030SikV4DTRK6VUgKswiV5EWorIuyIyV0QesDueshCRW0RkkojMFpE+dsdTFiLSSEQ+EJG5dsdSWiJSWUT+43wt7rY7nrLw59ehqAD7fHgmb3mjc76nH8Bk4CCwocj8fsAWYBsw1s19BQHTA6QsMcAHAVKWuXa/z0pbJmAocJNzerbdsXvi9fGV18FDZbH18+HhspQpb9leaDf/Y64EOrn+xwDBwHagERAGrAVaAW2BL4o8ajq3uRn4Chjs72VxbvdPoFOAlMUnEkwpy/Qk0MG5zky7Yy9LWXztdfBQWWz9fHiqLJ7IW35xc3BjzHci0rDI7C7ANmPMDgARmQX0N8a8DNx4gf0sABaIyJfATC+GfEGeKIuICDAe+MoYs8bLIV+Qp14XX1KaMgFpQAKQgg82g5ayLJvKObxSKU1ZRORXfODzcSGlfV08kbd87s1ZCvHAHpfnac55xRKRq0XkLRF5D1jo7eBKqVRlAR4GfgfcLiL3ezOwS1Da16WGiLwLdBSRJ70d3CW6UJk+BW4TkQl4+RJ2Dyq2LH7yOhR1odfFlz8fF3Kh18UjecsvavSeYIxZBiyzOQyPMMa8BbxldxyeYIw5AvjLh/EcxphTwHC74/AEf34digqwz8cyPJC3/LlGnw7Uc3me4Jznj7Qsvi2QyqRl8U1eLYs/J/pVQFMRSRSRMGAQsMDmmC6VlsW3BVKZtCy+ybtlsfsMtJtnqT8C9gE5WG1X9zrnXw9sxTpb/ZTdcWpZ/LcsgVgmLYtvPuwoiw5qppRSAc6fm26UUkq5QRO9UkoFOE30SikV4DTRK6VUgNNEr5RSAU4TvVJKBThN9EopFeA00StVCiJSYcaHUoFDL5hSFY5ziNivgdVY44JvBH4PPA7cBEQAy4H7jDFGRJZhDUV8BdZVjVuBcVjjhh8B7jbGHBCRZ4FErDHF6wNjgG7AdVjjltxkjMkpjzIq5Upr9Kqiag68Y4xpCZwAHgT+bYzpbIxpg5XsXcfPDzPGJBlj/gn8AHQzxnQEZgF/dlmvMdAL62YR04FvjTFtgTPADd4ulFLF0Z+hqqLaY4z50Tk9HfgjsFNE/gxEAtWxavr548zPdtk2AZgtInWwavU7XZZ9ZYzJEZH1WHcN+to5fz3Q0BsFUaokWqNXFVXRNksDvAPc7qyBTwLCXZafcpn+F1btvy1wX5H1zgIYYxxAjilsG3WgFStlE030qqKqLyKXO6cHYzXHABwWkSrA7RfZNprCscL/4KX4lPIYrWGoimoL8JCITMa6X+oEIAbYAOzHGh/8Qp4FPhaRY8BSrBOwSvks7XWjKhxnr5svnCddlQp42nSjlFIBTmv0SikV4LRGr5RSAU4TvVJKBThN9EopFeA00SulVIDTRK+UUgFOE71SSgW4/wfpYYyP0E1mvAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "q3 = MLQ3()\n",
        "param_df = q3.hyperparameter(X, y)\n",
        "param_df.plot(x=\"param\", y=[\"train_accuracy\", \"test_accuracy\"], logx=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngW4KMVLjJYP"
      },
      "source": [
        "[ The results indicate that the model achieves the highest performance on the test set when C equals 0.1. When C is less than 0.1, the model is subjected to excessive penalty which can cause underfitting. On the other hand, when C exceeds 0.1, it can result in overfitting. ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPsJTYu_jJYR"
      },
      "source": [
        "## Q4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sTI7Tc_GofkG"
      },
      "outputs": [],
      "source": [
        "class MLQ4():\n",
        "    def tf_improved_data(self, file_list, num_words = 1000):\n",
        "        # Put your code here\n",
        "        # Make sure you update the variable features and targets below\n",
        "        # Calculate word count in corpus\n",
        "        news_cnt = corpus_count_words_V2(file_list)\n",
        "    \n",
        "        # Select the most common numWords\n",
        "        word_list = [word for (word, freq) in news_cnt.most_common(num_words)]\n",
        "    \n",
        "        # Use several techniques to improve the text analyzing performance\n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        NewAnalyzer = RegexTokenizer(r'\\w+') | LowercaseFilter()| StopFilter() | StemFilter()| CustomFilter(WordNetLemmatizer().lemmatize)\n",
        "\n",
        "        df_rows = []\n",
        "        word_counter = Counter() # initialize an empty counter\n",
        "    \n",
        "        for file_path in file_list:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                file_data = file.read()\n",
        "                file_data = clean_file_text(file_data)\n",
        "\n",
        "                file_words = [token.text for token in NewAnalyzer(file_data)]\n",
        "    \n",
        "                word_counter.update(file_words)# Constructed the counter via update() method\n",
        "            \n",
        "                df_rows.append([word_counter[word] if word in file_words else 0 for word in word_list]) \n",
        "    \n",
        "        X = pd.DataFrame(df_rows, columns = word_list)\n",
        "        y = [get_target(get_topic_name(file_path)) for file_path in file_list]\n",
        "\n",
        "        # validate return types\n",
        "        assert isinstance(X, pd.DataFrame) and isinstance(y, list), \"return types\"\n",
        "\n",
        "        return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK7nTn82jJYS"
      },
      "source": [
        "### Q4 (a)\n",
        "\n",
        "Implement *tf_improved_data*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "yRlIduhLVvab",
        "outputId": "e769f1c2-520a-459c-802e-1e46673894b6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-29830346-462f-4f51-9205-fe56cd9ac222\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>edu</th>\n",
              "      <th>ax</th>\n",
              "      <th>com</th>\n",
              "      <th>wa</th>\n",
              "      <th>but</th>\n",
              "      <th>thei</th>\n",
              "      <th>line</th>\n",
              "      <th>new</th>\n",
              "      <th>messag</th>\n",
              "      <th>subject</th>\n",
              "      <th>...</th>\n",
              "      <th>archiv</th>\n",
              "      <th>abort</th>\n",
              "      <th>launch</th>\n",
              "      <th>worth</th>\n",
              "      <th>bring</th>\n",
              "      <th>du</th>\n",
              "      <th>shall</th>\n",
              "      <th>iastat</th>\n",
              "      <th>technic</th>\n",
              "      <th>famili</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>15</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19992</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36450</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23160</td>\n",
              "      <td>0</td>\n",
              "      <td>22307</td>\n",
              "      <td>22005</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19993</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36460</td>\n",
              "      <td>24550</td>\n",
              "      <td>23259</td>\n",
              "      <td>23201</td>\n",
              "      <td>23162</td>\n",
              "      <td>22868</td>\n",
              "      <td>22309</td>\n",
              "      <td>22006</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>776</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>771</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19994</th>\n",
              "      <td>66025</td>\n",
              "      <td>0</td>\n",
              "      <td>36464</td>\n",
              "      <td>24552</td>\n",
              "      <td>23261</td>\n",
              "      <td>23202</td>\n",
              "      <td>23163</td>\n",
              "      <td>22869</td>\n",
              "      <td>22310</td>\n",
              "      <td>22007</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>66028</td>\n",
              "      <td>0</td>\n",
              "      <td>36468</td>\n",
              "      <td>24554</td>\n",
              "      <td>23263</td>\n",
              "      <td>23206</td>\n",
              "      <td>23164</td>\n",
              "      <td>0</td>\n",
              "      <td>22311</td>\n",
              "      <td>22010</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>66031</td>\n",
              "      <td>0</td>\n",
              "      <td>36469</td>\n",
              "      <td>0</td>\n",
              "      <td>23264</td>\n",
              "      <td>23208</td>\n",
              "      <td>23165</td>\n",
              "      <td>0</td>\n",
              "      <td>22312</td>\n",
              "      <td>22011</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>772</td>\n",
              "      <td>771</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19997 rows Ã— 1000 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29830346-462f-4f51-9205-fe56cd9ac222')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-29830346-462f-4f51-9205-fe56cd9ac222 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-29830346-462f-4f51-9205-fe56cd9ac222');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         edu  ax    com     wa    but   thei   line    new  messag  subject  \\\n",
              "0          7   0      0      0      3     11      1      2       1        1   \n",
              "1         10   0      0      1      4      0      3      4       2        3   \n",
              "2         14   0      4      2      0      0      4      5       3        4   \n",
              "3         17   0      8      0      6     15      5      6       5        5   \n",
              "4         22   0     12      0      0     16      6      7       8        6   \n",
              "...      ...  ..    ...    ...    ...    ...    ...    ...     ...      ...   \n",
              "19992      0   0  36450      0      0      0  23160      0   22307    22005   \n",
              "19993      0   0  36460  24550  23259  23201  23162  22868   22309    22006   \n",
              "19994  66025   0  36464  24552  23261  23202  23163  22869   22310    22007   \n",
              "19995  66028   0  36468  24554  23263  23206  23164      0   22311    22010   \n",
              "19996  66031   0  36469      0  23264  23208  23165      0   22312    22011   \n",
              "\n",
              "       ...  archiv  abort  launch  worth  bring  du  shall  iastat  technic  \\\n",
              "0      ...       0      0       0      0      0   0      0       0        0   \n",
              "1      ...       0      0       0      0      0   0      0       0        0   \n",
              "2      ...       0      0       0      0      0   0      0       0        0   \n",
              "3      ...       0      0       0      0      1   0      0       0        0   \n",
              "4      ...       0      0       0      0      2   0      0       0        0   \n",
              "...    ...     ...    ...     ...    ...    ...  ..    ...     ...      ...   \n",
              "19992  ...       0      0       0      0      0   0      0       0        0   \n",
              "19993  ...       0      0       0      0    776   0      0       0      771   \n",
              "19994  ...       0      0       0      0      0   0      0       0        0   \n",
              "19995  ...       0      0       0      0      0   0      0       0        0   \n",
              "19996  ...       0      0       0      0      0   0      0       0      772   \n",
              "\n",
              "       famili  \n",
              "0           0  \n",
              "1           0  \n",
              "2           0  \n",
              "3           0  \n",
              "4           0  \n",
              "...       ...  \n",
              "19992       0  \n",
              "19993       0  \n",
              "19994       0  \n",
              "19995       0  \n",
              "19996     771  \n",
              "\n",
              "[19997 rows x 1000 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_tf, y_tf = MLQ4().tf_improved_data(all_files)\n",
        "X_tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Ws8ZgrjJYV"
      },
      "source": [
        "### Q4 (b)\n",
        "\n",
        "Use the following code to calculate the mean accuracy and 95% confidence interval over multiple random splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9c3b_RwjJYW",
        "outputId": "cef8e37d-537a-4b31-dfd4-e043294d025c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mean accuracy over 10 random splits: 0.7173608630420805\n",
            "Train confidence interval over 10 random splits: [0.7146099435374488, 0.7201117825467122]\n",
            "Test mean accuracy over 10 random splits: 0.6546833333333334\n",
            "Test confidence interval over 10 random splits: [0.6501348387448823, 0.6592318279217845]\n"
          ]
        }
      ],
      "source": [
        "q4 = MLQ4()\n",
        "X_tf, y_tf = q4.tf_improved_data(all_files)\n",
        "train_mean10, train_low10, train_high10, test_mean10, test_low10, test_high10 = MLQ1.random_mean_ci(X_tf, y_tf, num_tests = 10)\n",
        "print(\"Train mean accuracy over 10 random splits: {}\".format(train_mean10))\n",
        "print(\"Train confidence interval over 10 random splits: [{}, {}]\".format(train_low10, train_high10))\n",
        "print(\"Test mean accuracy over 10 random splits: {}\".format(test_mean10))\n",
        "print(\"Test confidence interval over 10 random splits: [{}, {}]\".format(test_low10, test_high10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKgIMSPRjJYc"
      },
      "source": [
        "[ The performance of TF encoding is observed to be inferior to that of binary coding on this dataset. The primary reason for this is that TF encoding assigns greater importance to words that occur more frequently, while binary encoding assigns equal weight to all words.\n",
        "\n",
        "Frequency-based encoding methods typically have a tendency to reduce noise and prioritize words that appear less frequently in the text. Additionally, TF encoding is heavily influenced by the specific corpus or dataset that is being used for training. For instance, if the training corpus contains a greater number of entertainment news articles, then the weight assigned to keywords related to entertainment may be lower. Therefore, selecting a high-quality corpus for training is crucial. ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAfgSoqjjJYd"
      },
      "source": [
        "## Q5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-z71UjpofkH"
      },
      "outputs": [],
      "source": [
        "class MLQ5():\n",
        "    def nb_random_mean_ci(self, X, y, num_tests):\n",
        "        # train_results is a list of train accuracy results for the differrent random splits of the dataset\n",
        "        train_results = []\n",
        "\n",
        "        # test_results is a list of test accuracy results for the differrent random splits of the dataset\n",
        "        test_results = []\n",
        "\n",
        "        # Write your code here\n",
        "        for i in range(0,num_tests):\n",
        "          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random.randint(1,1000))\n",
        "      \n",
        "          clf = MultinomialNB().fit(X_train,y_train)# multinomial naive bayes implements the naive bayes algorithm for multinomially distributed data\n",
        "\n",
        "          y_train_predict = clf.predict(X_train)\n",
        "          y_test_predict = clf.predict(X_test)\n",
        "\n",
        "          train_accuracy = accuracy_score(y_train,y_train_predict)\n",
        "          test_accuracy = accuracy_score(y_test,y_test_predict)\n",
        "\n",
        "          train_results.append(train_accuracy)\n",
        "          test_results.append(test_accuracy)\n",
        "    \n",
        "        # calculate the train mean and the 95% confidence interval for the list of results\n",
        "        train_mean = np.mean(train_results)\n",
        "        train_ci_low, train_ci_high = stats.t.interval(0.95, len(train_results)-1, loc=train_mean, scale=stats.sem(train_results))\n",
        "\n",
        "        # calculate the test mean and the 95% confidence interval for the list of results\n",
        "        test_mean = np.mean(test_results)\n",
        "        test_ci_low, test_ci_high = stats.t.interval(0.95, len(test_results)-1, loc=test_mean, scale=stats.sem(test_results))\n",
        "\n",
        "        # validate return types\n",
        "        assert isinstance(train_mean, float) and isinstance(train_ci_low, float) and isinstance(train_ci_high, float), \"return types\"\n",
        "        assert isinstance(test_mean, float) and isinstance(test_ci_low, float) and isinstance(test_ci_high, float), \"return types\"\n",
        "\n",
        "        return train_mean, train_ci_low, train_ci_high, test_mean, test_ci_low, test_ci_high\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tV_h9kGjJYg"
      },
      "source": [
        "### Q5 (a)\n",
        "\n",
        "Implement *nb_random_mean_ci*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9-OzAIUjJYj"
      },
      "source": [
        "### Q5 (b)\n",
        "\n",
        "Use the following code to calculate the mean accuracy and 95% confidence interval over multiple random splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9Me9H6HjJYk",
        "outputId": "60bc4938-e16a-4746-af59-ce7929194539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mean accuracy over 10 random splits: 0.7690719439879974\n",
            "Train confidence interval over 10 random splits: [0.7672646785564209, 0.7708792094195739]\n",
            "Test mean accuracy over 10 random splits: 0.7168166666666667\n",
            "Test confidence interval over 10 random splits: [0.7149488325803792, 0.7186845007529541]\n"
          ]
        }
      ],
      "source": [
        "q5 = MLQ5()\n",
        "train_mean10, train_low10, train_high10, test_mean10, test_low10, test_high10 = q5.nb_random_mean_ci(X, y, num_tests = 10)\n",
        "print(\"Train mean accuracy over 10 random splits: {}\".format(train_mean10))\n",
        "print(\"Train confidence interval over 10 random splits: [{}, {}]\".format(train_low10, train_high10))\n",
        "print(\"Test mean accuracy over 10 random splits: {}\".format(test_mean10))\n",
        "print(\"Test confidence interval over 10 random splits: [{}, {}]\".format(test_low10, test_high10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P99mWvAbjJYp"
      },
      "source": [
        "[ The accuracy results for Naive Bayes on both the training and testing sets have indicated a comparatively lower score than the logistic regression classifier. This may be attributed to the fact that Naive Bayes assumes independence among the features, whereas in our case, the features are generated from the most frequent words in the files, which may not be highly independent from each other. ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CPzZDj-jJYq"
      },
      "source": [
        "## Q6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdoCdzQqofkI"
      },
      "outputs": [],
      "source": [
        "class MLQ6():\n",
        "    def binary_med_data(self, file_list, num_words = 1000):\n",
        "        # Put your code here\n",
        "        # Make sure you update the variable features and targets below\n",
        "        # Corpus contains the word and its count\n",
        "        news_cnt = corpus_count_words_V2(file_list)\n",
        "    \n",
        "        # Word_list contains the most common numWords\n",
        "        word_list = [word for (word, freq) in news_cnt.most_common(num_words)]\n",
        "    \n",
        "        # Use several techniques to improve the text analyzing performance\n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        NewAnalyzer = RegexTokenizer(r'\\w+') | LowercaseFilter()| StopFilter() | StemFilter()| CustomFilter(WordNetLemmatizer().lemmatize)\n",
        "        \n",
        "        df_rows = []\n",
        "    \n",
        "        for file_path in file_list:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                file_data = file.read()\n",
        "                file_data = clean_file_text(file_data)\n",
        "                \n",
        "                file_words = [token.text for token in NewAnalyzer(file_data)]\n",
        "                df_rows.append([1 if word in file_words else 0 for word in word_list]) \n",
        "    \n",
        "        X = pd.DataFrame(df_rows, columns = word_list)\n",
        "        y = [1 if get_topic_name(file_path) == 'sci.med' else 0 for file_path in file_list]# if target name is sci,med returns 1 and retruns 0 for others\n",
        "\n",
        "        #Please remember to put index for your dataframe as the file name\n",
        "        #For example: pd.DataFrame(data, index=[str(f) for f in file_list],columns=[])\n",
        "\n",
        "        # validate return types\n",
        "        assert isinstance(X, pd.DataFrame) and isinstance(y, list), \"return types\"\n",
        "\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWL6L_OUjJYu"
      },
      "source": [
        "### Q6 (a)\n",
        "\n",
        "Implement *binary_med_data*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z3uKy8WjJYx"
      },
      "source": [
        "### Q6 (b)\n",
        "\n",
        "Use the following code to calculate the mean accuracy and 95% confidence interval over multiple random splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3Dfy0uIjJYy",
        "outputId": "6676ca61-3dfd-451e-d262-b2533b8eaa3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mean accuracy over 10 random splits: 0.9910409373437166\n",
            "Train confidence interval over 10 random splits: [0.9906269951173072, 0.9914548795701259]\n",
            "Test mean accuracy over 10 random splits: 0.9723166666666667\n",
            "Test confidence interval over 10 random splits: [0.9707475349618758, 0.9738857983714576]\n"
          ]
        }
      ],
      "source": [
        "q6 = MLQ6()\n",
        "X, y = q6.binary_med_data(all_files)\n",
        "train_mean10, train_low10, train_high10, test_mean10, test_low10, test_high10 = MLQ1.random_mean_ci(X, y, num_tests = 10)\n",
        "print(\"Train mean accuracy over 10 random splits: {}\".format(train_mean10))\n",
        "print(\"Train confidence interval over 10 random splits: [{}, {}]\".format(train_low10, train_high10))\n",
        "print(\"Test mean accuracy over 10 random splits: {}\".format(test_mean10))\n",
        "print(\"Test confidence interval over 10 random splits: [{}, {}]\".format(test_low10, test_high10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EueLolt1jJY0"
      },
      "source": [
        "[ The accuracy results for both the training and testing sets are exceptionally high (both above 97%) for binary classification in Q1, indicating significantly better performance than multiclass classification. However, it is important to note that the dataset suffers from a class imbalance issue where the classes are not equally represented. Therefore, accuracy may not be an appropriate metric to use for evaluating the model's performance as it can be highly misleading.\n",
        "\n",
        "To gain better insights into the accuracy of the model, I recommend using alternative performance measures such as a confusion matrix, precision (exactness), recall (completeness), and F1 score (a weighted average of precision and recall). These measures can provide a more comprehensive evaluation of the model's performance than traditional classification accuracy. ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXGeOS9VpJYS"
      },
      "source": [
        "# Q7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FW1IokMpLb9"
      },
      "source": [
        "## Q7(a)\n",
        "use the following code cell to implement your feature encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "mIT_KYwmpKbW"
      },
      "outputs": [],
      "source": [
        "def data_q7(file_list, num_words = 3000):\n",
        "  #write your code here, define your model\n",
        "  news_cnt = corpus_count_words_V2(file_list)\n",
        "    \n",
        "    # Select the most common numWords\n",
        "  word_list = [word for (word, freq) in news_cnt.most_common(num_words)]\n",
        "    \n",
        "  df_rows = []\n",
        "  for file_path in file_list:\n",
        "      with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "          file_data = file.read()\n",
        "          file_data = clean_file_text(file_data)\n",
        "          NewAnalyzer = RegexTokenizer(r'\\w+') | LowercaseFilter()| StopFilter() | StemFilter()| CustomFilter(WordNetLemmatizer().lemmatize)\n",
        "          file_words = [token.text for token in NewAnalyzer(file_data)]\n",
        "          df_rows.append([1 if word in file_words else 0 for word in word_list])\n",
        "  X = pd.DataFrame(df_rows, index=[str(f) for f in file_list], columns = word_list)\n",
        "  y = [get_target(get_topic_name(file_path)) for file_path in file_list]\n",
        "  # validate return types\n",
        "  assert isinstance(X, pd.DataFrame) and isinstance(y, list), \"return types\"\n",
        "  \n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igUVLrzppRGT"
      },
      "source": [
        "## Q7(b)\n",
        "Use the following code cell to implement your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "VdSJlJgrpXPh"
      },
      "outputs": [],
      "source": [
        "def build_model_q7():\n",
        "  #MODELQ7=LogisticRegression(C=0.2)\n",
        "  #MODELQ7=LogisticRegression(C=0.1, sovler = 'saga')\n",
        "  #MODELQ7=LogisticRegression(C=0.1, penalty = 'l2')\n",
        "  #MODELQ7=LogisticRegression(C=0.1)\n",
        "  #MODELQ7 = MultinomialNB()\n",
        "  #MODELQ7 = MultinomialNB(alpha=0.5)\n",
        "  #MODELQ7 = RandomForestClassifier()\n",
        "  #MODELQ7 = RandomForestClassifier(n_estimators=200)\n",
        "  MODELQ7 = MultinomialNB()\n",
        "\n",
        "  return MODELQ7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUAvw4aUpTJm"
      },
      "source": [
        "Code for evaluating p at k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "vliVevlZpaqe"
      },
      "outputs": [],
      "source": [
        "def calculate_average_precision_at_k(MODELQ7, all_files, training_files, testing_files,k=None):\n",
        "  \n",
        "  training_files = [str(f) for f in open(training_files,mode='r').read().splitlines()]\n",
        "  testing_files = [str(f) for f in open(testing_files,mode='r').read().splitlines()]\n",
        "  if k is None:\n",
        "    k=len(testing_files)\n",
        "\n",
        "  X, y = data_q7(all_files) \n",
        "  X['gt'] = y\n",
        "  training = X.loc[training_files]\n",
        "  X_train = training.loc[:,training.columns!='gt']\n",
        "  y_train = training['gt'].values\n",
        "\n",
        "  testing = X.loc[testing_files]\n",
        "  X_test = testing.loc[:,testing.columns!='gt']\n",
        "  y_test = testing['gt'].values\n",
        "\n",
        "  MODELQ7.fit(X_train,y_train)\n",
        "  y_pred = MODELQ7.predict(X_test)\n",
        "  y_pred_prob = MODELQ7.predict_proba(X_test)\n",
        "  confidences = np.max(y_pred_prob,axis=1)\n",
        "  \n",
        "  p_at_k = []\n",
        "  rel_at_k = []\n",
        "  confidence_order = np.argsort(confidences)\n",
        "  for i in range(1,k+1):\n",
        "    top_confidence = confidence_order[-i:]\n",
        "    pred_top_i = y_pred[top_confidence]\n",
        "    gt_top_i = np.array(y_test)[top_confidence]\n",
        "    p_at_i = np.sum(pred_top_i == gt_top_i)/i\n",
        "    rel_at_i = (pred_top_i[0] == gt_top_i[0])\n",
        "    p_at_k.append(p_at_i)\n",
        "    rel_at_k.append(rel_at_i)\n",
        "  print('average precision at {} is {}'.format(k,np.dot(p_at_k,rel_at_k)/k))\n",
        "  return np.dot(p_at_k,rel_at_k)/k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ3-RnnrpeSn",
        "outputId": "27842142-e279-4aad-d9fc-d2545343aba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average precision at 4000 is 0.6656129660065269\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "######This line of code must be able to run on Google Colab in under 15 minutes.#####\n",
        "######Code that runs longer than 15 minutes on the autograder will receive 0 marks for Q7#####\n",
        "m = calculate_average_precision_at_k(build_model_q7(), all_files, 'training_files_Q7.txt', 'testing_files_Q7.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR7_zkDfpgJQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLIjqlH6pgKk"
      },
      "source": [
        "# Q7(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvHHuoXkpjKM"
      },
      "source": [
        "Best model is 10th try, which uses improved binary encoding, default naive bayes model. The average precision at 4000 is 0.6656129660065269.\n",
        "\n",
        "1st Try: use 1200 num features(shown in q2 the more the features the better the model accuracy) with binary encoding(shown previously better than tf encoding),try logistic regression(shown previously better than naive bayes model) with hyperparameter c = 0.1(best in q3), the result is:average precision at 4000 is 0.37790170374070203.\n",
        "\n",
        "2nd Try: use 1200 num featrues with LowercaseFilter()| StopFilter() | StemFilter()| CustomFilter(WordNetLemmatizer().lemmatize) on the features with binary encoding, run the same logistic regression with c=0.1, the result is:average precision at 4000 is 0.5647925450363261. Which is significantly improved. \n",
        "\n",
        "3rd Try: try with 1500 num featrues with the rest same as 2nd try, the result is: average precision at 4000 is 0.6060675865694292. Improved again with more num of featrues.\n",
        "\n",
        "4th Try: tune the hyperparameter c to 0.2, the previous test only test it to be 0.01,0.1 and 1, the best c could any values between 0.01 to 1, with all the rest setting being the same as 3rd try. The result is:average precision at 4000 is 0.600248108419866, slightly lower than 3rd try, but not worth much effort turning hyperparameter c.\n",
        "\n",
        "5th Try, set the model hyperparameter c back to 0.1, use 1500 num of features and try logistic regression with l2 penalty to avoid overfitting on the training set. The result is: average precision at 4000 is 0.6060675865694292,exactly the same as 3rd one.\n",
        "\n",
        "6th try, remove the l2 penalty, try a different sovler saga (default is lbfgs), the result is:average precision at 4000 is 0.6057920420817018, slighly lower than lbfgs sovler by default, the change is not significant so it's not worth switching solvers.\n",
        "\n",
        "7th try, use 2000 num features, improved binary encoding, logistic regression, c = 0.1,it seems that the most useful improvement is the number of features. the result is:average precision at 4000 is 0.6352628122953884. significant improvement.\n",
        "\n",
        "8th try, use 3000 num featrues, the result is: average precision at 4000 is 0.6624690346825688, AP still improving with increased num features.\n",
        "\n",
        "9th try, use 1200 num features, try naive bayesion and compare the result with 2nd try. The result is: average precision at 4000 is 0.544194521954847, little lower than 2nd try. \n",
        "\n",
        "10th try, use 3000 num features, same naive beys model, the result is:average precision at 4000 is 0.6656129660065269. slightly higher than the 8th best logistic regression model. \n",
        "\n",
        "11th try, use 3000 num features, tune Laplace smoothing factor alpha = 0.5(less smoothing),the result is:average precision at 4000 is 0.6650433648940861, slightly lower than 10th try, not worth exploring.\n",
        "\n",
        "12th try, try 1200 num features with random forest classifier by default, compare result with 2nd try. The result is:average precision at 4000 is 0.5290052814797435, AP lower than 2nd try, still worth exploring.\n",
        "\n",
        "13th try, try n_estimators=200 with 1200 num features, the result is:average precision at 4000 is 0.5379813041818128, slightly improved.\n",
        "\n",
        "14th try, try n_estimator = 200 with 3000 num features, the result is: average precision at 4000 is 0.6088514663323488. Not as good as naive bayes or logistic regression.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq9ptAjfjF5T"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}